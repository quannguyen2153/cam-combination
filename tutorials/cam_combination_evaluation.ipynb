{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from typing import List\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_grad_cam as cam\n",
    "import pytorch_grad_cam.utils as utils\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, ClassifierOutputSoftmaxTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "\n",
    "def display_images_in_rows(rgb_img: np.ndarray, grayscale_list: List[torch.Tensor], labels: List[str]):\n",
    "    \"\"\"\n",
    "    Displays a list of images in rows, with each row containing up to max_images_per_row images.\n",
    "    \n",
    "    Args:\n",
    "        images (list of np.ndarray): List of images to display.\n",
    "        labels (list of str): List of labels for each image.\n",
    "        max_images_per_row (int): Maximum number of images per row (default is 4).\n",
    "    \"\"\"\n",
    "\n",
    "    images = [rgb_img]\n",
    "\n",
    "    for grayscale in grayscale_list:\n",
    "        images.append(show_cam_on_image(rgb_img, grayscale, use_rgb=True))\n",
    "\n",
    "    labels.insert(0, \"Image\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "    # Loop through the images and labels to display them\n",
    "    for ax, img, label in zip(axes, images, labels):\n",
    "        ax.imshow(img)  # Show the image\n",
    "        ax.set_title(label)  # Set the title\n",
    "        ax.axis('off')  # Hide axes\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "labels = json.load(open(\"../pytorch_grad_cam/utils/imagenet_class_index.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "input_folder = r\"../val_folder\"\n",
    "imputated_folder = \"../imputated_images\"\n",
    "\n",
    "if os.path.exists(imputated_folder):\n",
    "    shutil.rmtree(imputated_folder)\n",
    "os.makedirs(imputated_folder)\n",
    "\n",
    "# Generate a list of image names within the specified range\n",
    "start_idx = 1\n",
    "end_idx = 50000\n",
    "num_images_to_sample = 100  # Adjust this to how many random images you want\n",
    "\n",
    "all_image_names = [\n",
    "    f\"ILSVRC2012_val_{i:08d}.JPEG\" for i in range(start_idx, end_idx + 1)\n",
    "]\n",
    "random_image_names = random.sample(all_image_names, num_images_to_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_bb(img, bb):\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img, cmap='gray')\n",
    "\n",
    "    # Define the bounding box coordinates\n",
    "    x1, y1, x2, y2 = bb\n",
    "\n",
    "    # Calculate width and height\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    # Create a rectangle patch\n",
    "    rect = patches.Rectangle((x1, y1), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "\n",
    "    # Add the rectangle to the axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Show the image with the bounding box\n",
    "    plt.show()\n",
    "\n",
    "def filter_images(csv_file, images_folder, num_samples=None):\n",
    "    eligible_images = []\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data = data.sample(frac=1).reset_index(drop=True)  # Shuffle data for randomness upfront\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        image_id = row[\"ImageId\"]\n",
    "        predictions = row[\"PredictionString\"].split()\n",
    "\n",
    "        # Parse predictions into labels and coordinates\n",
    "        bboxes = []\n",
    "        for i in range(0, len(predictions), 5):\n",
    "            class_label = predictions[i]\n",
    "            coords = list(map(int, predictions[i + 1:i + 5]))\n",
    "            bboxes.append((class_label, coords))\n",
    "\n",
    "        # Skip if there are multiple bounding boxes\n",
    "        if len(bboxes) > 1:\n",
    "            continue\n",
    "\n",
    "        # Load the image to get dimensions\n",
    "        image_path = f\"{images_folder}/{image_id}.JPEG\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        # Calculate total bounding box area\n",
    "        total_bbox_area = sum(\n",
    "            (x_max - x_min) * (y_max - y_min)\n",
    "            for _, (x_min, y_min, x_max, y_max) in bboxes\n",
    "        )\n",
    "        image_area = img_width * img_height\n",
    "\n",
    "        # Skip if the object occupies 50% or more of the image\n",
    "        if total_bbox_area / image_area >= 0.5:\n",
    "            continue\n",
    "\n",
    "        # Add eligible image and bounding box to the list\n",
    "        eligible_images.append(row)\n",
    "\n",
    "        # Stop if we've collected enough samples\n",
    "        if num_samples and len(eligible_images) >= num_samples:\n",
    "            break\n",
    "\n",
    "    return eligible_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop in confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "total_drop_in_conf = 0\n",
    "total_increase_in_conf = 0\n",
    "num_images = 0  # Track number of processed images\n",
    "\n",
    "input_folder = r\"../val_folder\"\n",
    "random_image_names = ['ILSVRC2012_val_00048014.JPEG', 'ILSVRC2012_val_00040662.JPEG', 'ILSVRC2012_val_00021194.JPEG', 'ILSVRC2012_val_00038840.JPEG', 'ILSVRC2012_val_00026519.JPEG', 'ILSVRC2012_val_00026939.JPEG', 'ILSVRC2012_val_00000022.JPEG', 'ILSVRC2012_val_00000165.JPEG', 'ILSVRC2012_val_00028945.JPEG', 'ILSVRC2012_val_00032674.JPEG', 'ILSVRC2012_val_00026695.JPEG', 'ILSVRC2012_val_00043106.JPEG', 'ILSVRC2012_val_00038333.JPEG', 'ILSVRC2012_val_00018162.JPEG', 'ILSVRC2012_val_00004863.JPEG', 'ILSVRC2012_val_00034851.JPEG', 'ILSVRC2012_val_00004517.JPEG', 'ILSVRC2012_val_00010810.JPEG', 'ILSVRC2012_val_00011824.JPEG', 'ILSVRC2012_val_00020485.JPEG', 'ILSVRC2012_val_00048173.JPEG', 'ILSVRC2012_val_00041393.JPEG', 'ILSVRC2012_val_00033662.JPEG', 'ILSVRC2012_val_00045303.JPEG', 'ILSVRC2012_val_00004455.JPEG', 'ILSVRC2012_val_00011500.JPEG', 'ILSVRC2012_val_00025962.JPEG', 'ILSVRC2012_val_00043584.JPEG', 'ILSVRC2012_val_00036038.JPEG', 'ILSVRC2012_val_00001159.JPEG', 'ILSVRC2012_val_00036157.JPEG', 'ILSVRC2012_val_00047597.JPEG', 'ILSVRC2012_val_00044337.JPEG', 'ILSVRC2012_val_00003755.JPEG', 'ILSVRC2012_val_00043447.JPEG', 'ILSVRC2012_val_00031518.JPEG', 'ILSVRC2012_val_00041929.JPEG', 'ILSVRC2012_val_00010950.JPEG', 'ILSVRC2012_val_00023940.JPEG', 'ILSVRC2012_val_00034458.JPEG', 'ILSVRC2012_val_00003772.JPEG', 'ILSVRC2012_val_00017173.JPEG', 'ILSVRC2012_val_00035194.JPEG', 'ILSVRC2012_val_00013968.JPEG', 'ILSVRC2012_val_00007289.JPEG', 'ILSVRC2012_val_00035626.JPEG', 'ILSVRC2012_val_00001925.JPEG', 'ILSVRC2012_val_00018556.JPEG', 'ILSVRC2012_val_00005887.JPEG', 'ILSVRC2012_val_00037546.JPEG', 'ILSVRC2012_val_00037983.JPEG', 'ILSVRC2012_val_00028321.JPEG', 'ILSVRC2012_val_00006292.JPEG', 'ILSVRC2012_val_00010227.JPEG', 'ILSVRC2012_val_00020722.JPEG', 'ILSVRC2012_val_00010561.JPEG', 'ILSVRC2012_val_00040482.JPEG', 'ILSVRC2012_val_00042051.JPEG', 'ILSVRC2012_val_00001760.JPEG', 'ILSVRC2012_val_00021865.JPEG', 'ILSVRC2012_val_00010828.JPEG', 'ILSVRC2012_val_00043847.JPEG', 'ILSVRC2012_val_00036917.JPEG', 'ILSVRC2012_val_00047053.JPEG', 'ILSVRC2012_val_00002225.JPEG', 'ILSVRC2012_val_00014391.JPEG', 'ILSVRC2012_val_00023265.JPEG', 'ILSVRC2012_val_00025722.JPEG', 'ILSVRC2012_val_00035266.JPEG', 'ILSVRC2012_val_00000334.JPEG', 'ILSVRC2012_val_00009316.JPEG', 'ILSVRC2012_val_00037959.JPEG', 'ILSVRC2012_val_00015267.JPEG', 'ILSVRC2012_val_00045274.JPEG', 'ILSVRC2012_val_00005621.JPEG', 'ILSVRC2012_val_00009324.JPEG', 'ILSVRC2012_val_00036612.JPEG', 'ILSVRC2012_val_00012167.JPEG', 'ILSVRC2012_val_00013826.JPEG', 'ILSVRC2012_val_00039615.JPEG', 'ILSVRC2012_val_00003550.JPEG', 'ILSVRC2012_val_00018661.JPEG', 'ILSVRC2012_val_00037578.JPEG', 'ILSVRC2012_val_00032692.JPEG', 'ILSVRC2012_val_00022024.JPEG', 'ILSVRC2012_val_00011285.JPEG', 'ILSVRC2012_val_00017859.JPEG', 'ILSVRC2012_val_00025713.JPEG', 'ILSVRC2012_val_00027390.JPEG', 'ILSVRC2012_val_00045695.JPEG', 'ILSVRC2012_val_00038690.JPEG', 'ILSVRC2012_val_00016934.JPEG', 'ILSVRC2012_val_00027410.JPEG', 'ILSVRC2012_val_00039936.JPEG', 'ILSVRC2012_val_00025764.JPEG', 'ILSVRC2012_val_00024909.JPEG', 'ILSVRC2012_val_00003979.JPEG', 'ILSVRC2012_val_00035948.JPEG', 'ILSVRC2012_val_00044730.JPEG', 'ILSVRC2012_val_00041283.JPEG']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).eval().to(device)\n",
    "target_layers = [model.features[10]]\n",
    "\n",
    "cam_metric = utils.CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = utils.DropInConfidence()\n",
    "increase_in_conf_metric = utils.IncreaseInConfidence()\n",
    "\n",
    "# Process the randomly selected images\n",
    "for filename in random_image_names:\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    if os.path.exists(img_path):  # Ensure the file exists\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.float32(img) / 255\n",
    "        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_class = model(input_tensor).max(1)[-1]\n",
    "            targets = [ClassifierOutputTarget(predicted_class)]\n",
    "            print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "        grad_cam_pp_grayscale = cam.GradCAMPlusPlus(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "        score_cam_grayscale = cam.ScoreCAM(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "        gray_scale = utils.combine_by_matching_important_pixels(grayscales=[grad_cam_pp_grayscale[0, :], score_cam_grayscale[0, :]], thresholds=[25, 25])\n",
    "        gray_scale = np.expand_dims(gray_scale, axis=0)\n",
    "\n",
    "        threshold = np.percentile(gray_scale, 50)\n",
    "        gray_scale[gray_scale < threshold] = 0\n",
    "        \n",
    "        # display_images_in_rows(rgb_img=img, grayscale_list=[grad_cam_pp_grayscale[0, :], score_cam_grayscale[0, :], gray_scale[0, :]], labels=[\"GradCAM++\", \"ScoreCAM\", \"MatchingCAM\"])\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Calculate Drop in Confidence and Increase in Confidence\n",
    "        drop_in_conf = drop_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "        increase_in_conf = increase_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "\n",
    "        # Accumulate the scores for averaging\n",
    "        total_drop_in_conf += drop_in_conf\n",
    "        total_increase_in_conf += increase_in_conf\n",
    "        num_images += 1  # Increment image count\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        score = scores[0]\n",
    "        visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "\n",
    "        # Print individual results\n",
    "        print(f\"Image: {filename}\")\n",
    "        print(f\"Confidence before imputation: {scores_before}\")\n",
    "        print(f\"Confidence after imputation: {scores_after}\")\n",
    "        print(f\"The confidence increase raw: {score}\")\n",
    "        print(f\"The drop in confidence percentage: {drop_in_conf}%\")\n",
    "        print(f\"The increase in confidence: {increase_in_conf}\")\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "# Calculate and print averages after processing all images\n",
    "if num_images > 0:\n",
    "    avg_drop_in_conf = total_drop_in_conf / num_images\n",
    "    avg_increase_in_conf = total_increase_in_conf / num_images * 100\n",
    "    print(f\"\\nNumber of images processed: {num_images}\")\n",
    "    print(f\"Average Drop in Confidence: {avg_drop_in_conf}%\")\n",
    "    print(f\"Average Increase in Confidence: {avg_increase_in_conf}%\")\n",
    "else:\n",
    "    print(\"No valid images found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "tensor([605])\n",
      "Target class: iPod\n",
      "tensor([748])\n",
      "Target class: purse\n",
      "tensor([898])\n",
      "Target class: water_bottle\n",
      "tensor([760])\n",
      "Target class: refrigerator\n",
      "tensor([980])\n",
      "Target class: volcano\n",
      "tensor([126])\n",
      "Target class: isopod\n",
      "tensor([209])\n",
      "Target class: Chesapeake_Bay_retriever\n",
      "tensor([937])\n",
      "Target class: broccoli\n",
      "tensor([203])\n",
      "Target class: West_Highland_white_terrier\n",
      "tensor([954])\n",
      "Target class: banana\n",
      "tensor([561])\n",
      "Target class: forklift\n",
      "tensor([850])\n",
      "Target class: teddy\n",
      "tensor([495])\n",
      "Target class: china_cabinet\n",
      "tensor([12])\n",
      "Target class: house_finch\n",
      "tensor([694])\n",
      "Target class: paddlewheel\n",
      "tensor([248])\n",
      "Target class: Eskimo_dog\n",
      "tensor([305])\n",
      "Target class: dung_beetle\n",
      "tensor([98])\n",
      "Target class: red-breasted_merganser\n",
      "tensor([763])\n",
      "Target class: revolver\n",
      "tensor([746])\n",
      "Target class: puck\n",
      "tensor([735])\n",
      "Target class: poncho\n",
      "tensor([211])\n",
      "Target class: vizsla\n",
      "tensor([807])\n",
      "Target class: solar_dish\n",
      "tensor([653])\n",
      "Target class: milk_can\n",
      "tensor([586])\n",
      "Target class: half_track\n",
      "tensor([962])\n",
      "Target class: meat_loaf\n",
      "tensor([641])\n",
      "Target class: maraca\n",
      "tensor([536])\n",
      "Target class: dock\n",
      "tensor([695])\n",
      "Target class: padlock\n",
      "tensor([958])\n",
      "Target class: hay\n",
      "tensor([6])\n",
      "Target class: stingray\n",
      "tensor([267])\n",
      "Target class: standard_poodle\n",
      "tensor([597])\n",
      "Target class: holster\n",
      "tensor([458])\n",
      "Target class: brass\n",
      "tensor([388])\n",
      "Target class: giant_panda\n",
      "tensor([721])\n",
      "Target class: pillow\n",
      "tensor([232])\n",
      "Target class: Border_collie\n",
      "tensor([792])\n",
      "Target class: shovel\n",
      "tensor([313])\n",
      "Target class: walking_stick\n",
      "tensor([703])\n",
      "Target class: park_bench\n",
      "tensor([752])\n",
      "Target class: racket\n",
      "tensor([658])\n",
      "Target class: mitten\n",
      "tensor([551])\n",
      "Target class: face_powder\n",
      "tensor([463])\n",
      "Target class: bucket\n",
      "tensor([251])\n",
      "Target class: dalmatian\n",
      "tensor([741])\n",
      "Target class: prayer_rug\n",
      "tensor([262])\n",
      "Target class: Brabancon_griffon\n",
      "tensor([218])\n",
      "Target class: Welsh_springer_spaniel\n",
      "tensor([966])\n",
      "Target class: red_wine\n",
      "tensor([237])\n",
      "Target class: miniature_pinscher\n",
      "tensor([363])\n",
      "Target class: armadillo\n",
      "tensor([216])\n",
      "Target class: clumber\n",
      "tensor([391])\n",
      "Target class: coho\n",
      "tensor([670])\n",
      "Target class: motor_scooter\n",
      "tensor([835])\n",
      "Target class: sundial\n",
      "tensor([758])\n",
      "Target class: reel\n",
      "tensor([841])\n",
      "Target class: sweatshirt\n",
      "tensor([832])\n",
      "Target class: stupa\n",
      "tensor([78])\n",
      "Target class: tick\n",
      "tensor([473])\n",
      "Target class: can_opener\n",
      "tensor([450])\n",
      "Target class: bobsled\n",
      "tensor([527])\n",
      "Target class: desktop_computer\n",
      "tensor([811])\n",
      "Target class: space_heater\n",
      "tensor([621])\n",
      "Target class: lawn_mower\n",
      "tensor([319])\n",
      "Target class: dragonfly\n",
      "tensor([361])\n",
      "Target class: skunk\n",
      "tensor([628])\n",
      "Target class: liner\n",
      "tensor([113])\n",
      "Target class: snail\n",
      "tensor([491])\n",
      "Target class: chain_saw\n",
      "tensor([616])\n",
      "Target class: knot\n",
      "tensor([799])\n",
      "Target class: sliding_door\n",
      "tensor([980])\n",
      "Target class: volcano\n",
      "tensor([470])\n",
      "Target class: candle\n",
      "tensor([587])\n",
      "Target class: hammer\n",
      "tensor([357])\n",
      "Target class: mink\n",
      "tensor([192])\n",
      "Target class: cairn\n",
      "tensor([373])\n",
      "Target class: macaque\n",
      "tensor([365])\n",
      "Target class: orangutan\n",
      "tensor([661])\n",
      "Target class: Model_T\n",
      "tensor([882])\n",
      "Target class: vacuum\n",
      "tensor([276])\n",
      "Target class: hyena\n",
      "tensor([575])\n",
      "Target class: golfcart\n",
      "tensor([550])\n",
      "Target class: espresso_maker\n",
      "tensor([829])\n",
      "Target class: streetcar\n",
      "tensor([665])\n",
      "Target class: moped\n",
      "tensor([611])\n",
      "Target class: jigsaw_puzzle\n",
      "tensor([822])\n",
      "Target class: steel_drum\n",
      "tensor([971])\n",
      "Target class: bubble\n",
      "tensor([635])\n",
      "Target class: magnetic_compass\n",
      "tensor([945])\n",
      "Target class: bell_pepper\n",
      "tensor([255])\n",
      "Target class: Leonberg\n",
      "tensor([26])\n",
      "Target class: common_newt\n",
      "tensor([983])\n",
      "Target class: scuba_diver\n",
      "tensor([355])\n",
      "Target class: llama\n",
      "tensor([100])\n",
      "Target class: black_swan\n",
      "tensor([443])\n",
      "Target class: bib\n",
      "tensor([863])\n",
      "Target class: totem_pole\n",
      "tensor([902])\n",
      "Target class: whistle\n",
      "tensor([219])\n",
      "Target class: cocker_spaniel\n",
      "tensor([839])\n",
      "Target class: suspension_bridge\n",
      "\n",
      "Number of images processed: 100\n",
      "Average Drop in Confidence: [[45.65416]]%\n",
      "Average Increase in Confidence: [[10.]]%\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics\n",
    "total_drop_in_conf = 0\n",
    "total_increase_in_conf = 0\n",
    "num_images = 0  # Track number of processed images\n",
    "\n",
    "# input_folder = r\"../val_folder\"\n",
    "input_folder = r\"C:\\Users\\HaPham\\Documents\\ThesisXAI\\Code\\CAM-combination\\ILSVRC2012\\ILSVRC2012_img_val\"\n",
    "\n",
    "random_image_names = ['ILSVRC2012_val_00048014.JPEG', 'ILSVRC2012_val_00040662.JPEG', 'ILSVRC2012_val_00021194.JPEG', 'ILSVRC2012_val_00038840.JPEG', 'ILSVRC2012_val_00026519.JPEG', 'ILSVRC2012_val_00026939.JPEG', 'ILSVRC2012_val_00000022.JPEG', 'ILSVRC2012_val_00000165.JPEG', 'ILSVRC2012_val_00028945.JPEG', 'ILSVRC2012_val_00032674.JPEG', 'ILSVRC2012_val_00026695.JPEG', 'ILSVRC2012_val_00043106.JPEG', 'ILSVRC2012_val_00038333.JPEG', 'ILSVRC2012_val_00018162.JPEG', 'ILSVRC2012_val_00004863.JPEG', 'ILSVRC2012_val_00034851.JPEG', 'ILSVRC2012_val_00004517.JPEG', 'ILSVRC2012_val_00010810.JPEG', 'ILSVRC2012_val_00011824.JPEG', 'ILSVRC2012_val_00020485.JPEG', 'ILSVRC2012_val_00048173.JPEG', 'ILSVRC2012_val_00041393.JPEG', 'ILSVRC2012_val_00033662.JPEG', 'ILSVRC2012_val_00045303.JPEG', 'ILSVRC2012_val_00004455.JPEG', 'ILSVRC2012_val_00011500.JPEG', 'ILSVRC2012_val_00025962.JPEG', 'ILSVRC2012_val_00043584.JPEG', 'ILSVRC2012_val_00036038.JPEG', 'ILSVRC2012_val_00001159.JPEG', 'ILSVRC2012_val_00036157.JPEG', 'ILSVRC2012_val_00047597.JPEG', 'ILSVRC2012_val_00044337.JPEG', 'ILSVRC2012_val_00003755.JPEG', 'ILSVRC2012_val_00043447.JPEG', 'ILSVRC2012_val_00031518.JPEG', 'ILSVRC2012_val_00041929.JPEG', 'ILSVRC2012_val_00010950.JPEG', 'ILSVRC2012_val_00023940.JPEG', 'ILSVRC2012_val_00034458.JPEG', 'ILSVRC2012_val_00003772.JPEG', 'ILSVRC2012_val_00017173.JPEG', 'ILSVRC2012_val_00035194.JPEG', 'ILSVRC2012_val_00013968.JPEG', 'ILSVRC2012_val_00007289.JPEG', 'ILSVRC2012_val_00035626.JPEG', 'ILSVRC2012_val_00001925.JPEG', 'ILSVRC2012_val_00018556.JPEG', 'ILSVRC2012_val_00005887.JPEG', 'ILSVRC2012_val_00037546.JPEG', 'ILSVRC2012_val_00037983.JPEG', 'ILSVRC2012_val_00028321.JPEG', 'ILSVRC2012_val_00006292.JPEG', 'ILSVRC2012_val_00010227.JPEG', 'ILSVRC2012_val_00020722.JPEG', 'ILSVRC2012_val_00010561.JPEG', 'ILSVRC2012_val_00040482.JPEG', 'ILSVRC2012_val_00042051.JPEG', 'ILSVRC2012_val_00001760.JPEG', 'ILSVRC2012_val_00021865.JPEG', 'ILSVRC2012_val_00010828.JPEG', 'ILSVRC2012_val_00043847.JPEG', 'ILSVRC2012_val_00036917.JPEG', 'ILSVRC2012_val_00047053.JPEG', 'ILSVRC2012_val_00002225.JPEG', 'ILSVRC2012_val_00014391.JPEG', 'ILSVRC2012_val_00023265.JPEG', 'ILSVRC2012_val_00025722.JPEG', 'ILSVRC2012_val_00035266.JPEG', 'ILSVRC2012_val_00000334.JPEG', 'ILSVRC2012_val_00009316.JPEG', 'ILSVRC2012_val_00037959.JPEG', 'ILSVRC2012_val_00015267.JPEG', 'ILSVRC2012_val_00045274.JPEG', 'ILSVRC2012_val_00005621.JPEG', 'ILSVRC2012_val_00009324.JPEG', 'ILSVRC2012_val_00036612.JPEG', 'ILSVRC2012_val_00012167.JPEG', 'ILSVRC2012_val_00013826.JPEG', 'ILSVRC2012_val_00039615.JPEG', 'ILSVRC2012_val_00003550.JPEG', 'ILSVRC2012_val_00018661.JPEG', 'ILSVRC2012_val_00037578.JPEG', 'ILSVRC2012_val_00032692.JPEG', 'ILSVRC2012_val_00022024.JPEG', 'ILSVRC2012_val_00011285.JPEG', 'ILSVRC2012_val_00017859.JPEG', 'ILSVRC2012_val_00025713.JPEG', 'ILSVRC2012_val_00027390.JPEG', 'ILSVRC2012_val_00045695.JPEG', 'ILSVRC2012_val_00038690.JPEG', 'ILSVRC2012_val_00016934.JPEG', 'ILSVRC2012_val_00027410.JPEG', 'ILSVRC2012_val_00039936.JPEG', 'ILSVRC2012_val_00025764.JPEG', 'ILSVRC2012_val_00024909.JPEG', 'ILSVRC2012_val_00003979.JPEG', 'ILSVRC2012_val_00035948.JPEG', 'ILSVRC2012_val_00044730.JPEG', 'ILSVRC2012_val_00041283.JPEG']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).eval().to(device)\n",
    "# target_layers = [model.features[10]]\n",
    "\n",
    "model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).eval()\n",
    "target_layers = [model.features[28]]\n",
    "\n",
    "cam_metric = utils.CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = utils.DropInConfidence()\n",
    "increase_in_conf_metric = utils.IncreaseInConfidence()\n",
    "\n",
    "# Process the randomly selected images\n",
    "for filename in random_image_names:\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    if os.path.exists(img_path):  # Ensure the file exists\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.float32(img) / 255\n",
    "        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_class = model(input_tensor).max(1)[-1]\n",
    "            targets = [ClassifierOutputTarget(predicted_class)]\n",
    "            print(predicted_class)\n",
    "            print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "        gray_scale = cam.GradCAM(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "        \n",
    "        threshold = np.percentile(gray_scale, 50)\n",
    "        gray_scale[gray_scale < threshold] = 0\n",
    "        \n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Calculate Drop in Confidence and Increase in Confidence\n",
    "        drop_in_conf = drop_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "        increase_in_conf = increase_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "\n",
    "        # Accumulate the scores for averaging\n",
    "        total_drop_in_conf += drop_in_conf\n",
    "        total_increase_in_conf += increase_in_conf\n",
    "        num_images += 1  # Increment image count\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        score = scores[0]\n",
    "        visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "\n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        # print(f\"Confidence before imputation: {scores_before}\")\n",
    "        # print(f\"Confidence after imputation: {scores_after}\")\n",
    "        # print(f\"The confidence increase raw: {score}\")\n",
    "        # print(f\"The drop in confidence percentage: {drop_in_conf}%\")\n",
    "        # print(f\"The increase in confidence: {increase_in_conf}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "\n",
    "# Calculate and print averages after processing all images\n",
    "if num_images > 0:\n",
    "    avg_drop_in_conf = total_drop_in_conf / num_images\n",
    "    avg_increase_in_conf = total_increase_in_conf / num_images * 100\n",
    "    print(f\"\\nNumber of images processed: {num_images}\")\n",
    "    print(f\"Average Drop in Confidence: {avg_drop_in_conf}%\")\n",
    "    print(f\"Average Increase in Confidence: {avg_increase_in_conf}%\")\n",
    "else:\n",
    "    print(\"No valid images found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "tensor([605])\n",
      "Target class: iPod\n",
      "tensor([748])\n",
      "Target class: purse\n",
      "tensor([898])\n",
      "Target class: water_bottle\n",
      "tensor([760])\n",
      "Target class: refrigerator\n",
      "tensor([980])\n",
      "Target class: volcano\n",
      "tensor([126])\n",
      "Target class: isopod\n",
      "tensor([209])\n",
      "Target class: Chesapeake_Bay_retriever\n",
      "tensor([937])\n",
      "Target class: broccoli\n",
      "tensor([203])\n",
      "Target class: West_Highland_white_terrier\n",
      "tensor([954])\n",
      "Target class: banana\n",
      "tensor([561])\n",
      "Target class: forklift\n",
      "tensor([850])\n",
      "Target class: teddy\n",
      "tensor([495])\n",
      "Target class: china_cabinet\n",
      "tensor([12])\n",
      "Target class: house_finch\n",
      "tensor([694])\n",
      "Target class: paddlewheel\n",
      "tensor([248])\n",
      "Target class: Eskimo_dog\n",
      "tensor([305])\n",
      "Target class: dung_beetle\n",
      "tensor([98])\n",
      "Target class: red-breasted_merganser\n",
      "tensor([763])\n",
      "Target class: revolver\n",
      "tensor([746])\n",
      "Target class: puck\n",
      "tensor([735])\n",
      "Target class: poncho\n",
      "tensor([211])\n",
      "Target class: vizsla\n",
      "tensor([807])\n",
      "Target class: solar_dish\n",
      "tensor([653])\n",
      "Target class: milk_can\n",
      "tensor([586])\n",
      "Target class: half_track\n",
      "tensor([962])\n",
      "Target class: meat_loaf\n",
      "tensor([641])\n",
      "Target class: maraca\n",
      "tensor([536])\n",
      "Target class: dock\n",
      "tensor([695])\n",
      "Target class: padlock\n",
      "tensor([958])\n",
      "Target class: hay\n",
      "tensor([6])\n",
      "Target class: stingray\n",
      "tensor([267])\n",
      "Target class: standard_poodle\n",
      "tensor([597])\n",
      "Target class: holster\n",
      "tensor([458])\n",
      "Target class: brass\n",
      "tensor([388])\n",
      "Target class: giant_panda\n",
      "tensor([721])\n",
      "Target class: pillow\n",
      "tensor([232])\n",
      "Target class: Border_collie\n",
      "tensor([792])\n",
      "Target class: shovel\n",
      "tensor([313])\n",
      "Target class: walking_stick\n",
      "tensor([703])\n",
      "Target class: park_bench\n",
      "tensor([752])\n",
      "Target class: racket\n",
      "tensor([658])\n",
      "Target class: mitten\n",
      "tensor([551])\n",
      "Target class: face_powder\n",
      "tensor([463])\n",
      "Target class: bucket\n",
      "tensor([251])\n",
      "Target class: dalmatian\n",
      "tensor([741])\n",
      "Target class: prayer_rug\n",
      "tensor([262])\n",
      "Target class: Brabancon_griffon\n",
      "tensor([218])\n",
      "Target class: Welsh_springer_spaniel\n",
      "tensor([966])\n",
      "Target class: red_wine\n",
      "tensor([237])\n",
      "Target class: miniature_pinscher\n",
      "tensor([363])\n",
      "Target class: armadillo\n",
      "tensor([216])\n",
      "Target class: clumber\n",
      "tensor([391])\n",
      "Target class: coho\n",
      "tensor([670])\n",
      "Target class: motor_scooter\n",
      "tensor([835])\n",
      "Target class: sundial\n",
      "tensor([758])\n",
      "Target class: reel\n",
      "tensor([841])\n",
      "Target class: sweatshirt\n",
      "tensor([832])\n",
      "Target class: stupa\n",
      "tensor([78])\n",
      "Target class: tick\n",
      "tensor([473])\n",
      "Target class: can_opener\n",
      "tensor([450])\n",
      "Target class: bobsled\n",
      "tensor([527])\n",
      "Target class: desktop_computer\n",
      "tensor([811])\n",
      "Target class: space_heater\n",
      "tensor([621])\n",
      "Target class: lawn_mower\n",
      "tensor([319])\n",
      "Target class: dragonfly\n",
      "tensor([361])\n",
      "Target class: skunk\n",
      "tensor([628])\n",
      "Target class: liner\n",
      "tensor([113])\n",
      "Target class: snail\n",
      "tensor([491])\n",
      "Target class: chain_saw\n",
      "tensor([616])\n",
      "Target class: knot\n",
      "tensor([799])\n",
      "Target class: sliding_door\n",
      "tensor([980])\n",
      "Target class: volcano\n",
      "tensor([470])\n",
      "Target class: candle\n",
      "tensor([587])\n",
      "Target class: hammer\n",
      "tensor([357])\n",
      "Target class: mink\n",
      "tensor([192])\n",
      "Target class: cairn\n",
      "tensor([373])\n",
      "Target class: macaque\n",
      "tensor([365])\n",
      "Target class: orangutan\n",
      "tensor([661])\n",
      "Target class: Model_T\n",
      "tensor([882])\n",
      "Target class: vacuum\n",
      "tensor([276])\n",
      "Target class: hyena\n",
      "tensor([575])\n",
      "Target class: golfcart\n",
      "tensor([550])\n",
      "Target class: espresso_maker\n",
      "tensor([829])\n",
      "Target class: streetcar\n",
      "tensor([665])\n",
      "Target class: moped\n",
      "tensor([611])\n",
      "Target class: jigsaw_puzzle\n",
      "tensor([822])\n",
      "Target class: steel_drum\n",
      "tensor([971])\n",
      "Target class: bubble\n",
      "tensor([635])\n",
      "Target class: magnetic_compass\n",
      "tensor([945])\n",
      "Target class: bell_pepper\n",
      "tensor([255])\n",
      "Target class: Leonberg\n",
      "tensor([26])\n",
      "Target class: common_newt\n",
      "tensor([983])\n",
      "Target class: scuba_diver\n",
      "tensor([355])\n",
      "Target class: llama\n",
      "tensor([100])\n",
      "Target class: black_swan\n",
      "tensor([443])\n",
      "Target class: bib\n",
      "tensor([863])\n",
      "Target class: totem_pole\n",
      "tensor([902])\n",
      "Target class: whistle\n",
      "tensor([219])\n",
      "Target class: cocker_spaniel\n",
      "tensor([839])\n",
      "Target class: suspension_bridge\n",
      "\n",
      "Number of images processed: 100\n",
      "Average Drop in Confidence: [[45.53167]]%\n",
      "Average Increase in Confidence: [[10.]]%\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics\n",
    "total_drop_in_conf = 0\n",
    "total_increase_in_conf = 0\n",
    "num_images = 0  # Track number of processed images\n",
    "\n",
    "# input_folder = r\"../val_folder\"\n",
    "input_folder = r\"C:\\Users\\HaPham\\Documents\\ThesisXAI\\Code\\CAM-combination\\ILSVRC2012\\ILSVRC2012_img_val\"\n",
    "random_image_names = ['ILSVRC2012_val_00048014.JPEG', 'ILSVRC2012_val_00040662.JPEG', 'ILSVRC2012_val_00021194.JPEG', 'ILSVRC2012_val_00038840.JPEG', 'ILSVRC2012_val_00026519.JPEG', 'ILSVRC2012_val_00026939.JPEG', 'ILSVRC2012_val_00000022.JPEG', 'ILSVRC2012_val_00000165.JPEG', 'ILSVRC2012_val_00028945.JPEG', 'ILSVRC2012_val_00032674.JPEG', 'ILSVRC2012_val_00026695.JPEG', 'ILSVRC2012_val_00043106.JPEG', 'ILSVRC2012_val_00038333.JPEG', 'ILSVRC2012_val_00018162.JPEG', 'ILSVRC2012_val_00004863.JPEG', 'ILSVRC2012_val_00034851.JPEG', 'ILSVRC2012_val_00004517.JPEG', 'ILSVRC2012_val_00010810.JPEG', 'ILSVRC2012_val_00011824.JPEG', 'ILSVRC2012_val_00020485.JPEG', 'ILSVRC2012_val_00048173.JPEG', 'ILSVRC2012_val_00041393.JPEG', 'ILSVRC2012_val_00033662.JPEG', 'ILSVRC2012_val_00045303.JPEG', 'ILSVRC2012_val_00004455.JPEG', 'ILSVRC2012_val_00011500.JPEG', 'ILSVRC2012_val_00025962.JPEG', 'ILSVRC2012_val_00043584.JPEG', 'ILSVRC2012_val_00036038.JPEG', 'ILSVRC2012_val_00001159.JPEG', 'ILSVRC2012_val_00036157.JPEG', 'ILSVRC2012_val_00047597.JPEG', 'ILSVRC2012_val_00044337.JPEG', 'ILSVRC2012_val_00003755.JPEG', 'ILSVRC2012_val_00043447.JPEG', 'ILSVRC2012_val_00031518.JPEG', 'ILSVRC2012_val_00041929.JPEG', 'ILSVRC2012_val_00010950.JPEG', 'ILSVRC2012_val_00023940.JPEG', 'ILSVRC2012_val_00034458.JPEG', 'ILSVRC2012_val_00003772.JPEG', 'ILSVRC2012_val_00017173.JPEG', 'ILSVRC2012_val_00035194.JPEG', 'ILSVRC2012_val_00013968.JPEG', 'ILSVRC2012_val_00007289.JPEG', 'ILSVRC2012_val_00035626.JPEG', 'ILSVRC2012_val_00001925.JPEG', 'ILSVRC2012_val_00018556.JPEG', 'ILSVRC2012_val_00005887.JPEG', 'ILSVRC2012_val_00037546.JPEG', 'ILSVRC2012_val_00037983.JPEG', 'ILSVRC2012_val_00028321.JPEG', 'ILSVRC2012_val_00006292.JPEG', 'ILSVRC2012_val_00010227.JPEG', 'ILSVRC2012_val_00020722.JPEG', 'ILSVRC2012_val_00010561.JPEG', 'ILSVRC2012_val_00040482.JPEG', 'ILSVRC2012_val_00042051.JPEG', 'ILSVRC2012_val_00001760.JPEG', 'ILSVRC2012_val_00021865.JPEG', 'ILSVRC2012_val_00010828.JPEG', 'ILSVRC2012_val_00043847.JPEG', 'ILSVRC2012_val_00036917.JPEG', 'ILSVRC2012_val_00047053.JPEG', 'ILSVRC2012_val_00002225.JPEG', 'ILSVRC2012_val_00014391.JPEG', 'ILSVRC2012_val_00023265.JPEG', 'ILSVRC2012_val_00025722.JPEG', 'ILSVRC2012_val_00035266.JPEG', 'ILSVRC2012_val_00000334.JPEG', 'ILSVRC2012_val_00009316.JPEG', 'ILSVRC2012_val_00037959.JPEG', 'ILSVRC2012_val_00015267.JPEG', 'ILSVRC2012_val_00045274.JPEG', 'ILSVRC2012_val_00005621.JPEG', 'ILSVRC2012_val_00009324.JPEG', 'ILSVRC2012_val_00036612.JPEG', 'ILSVRC2012_val_00012167.JPEG', 'ILSVRC2012_val_00013826.JPEG', 'ILSVRC2012_val_00039615.JPEG', 'ILSVRC2012_val_00003550.JPEG', 'ILSVRC2012_val_00018661.JPEG', 'ILSVRC2012_val_00037578.JPEG', 'ILSVRC2012_val_00032692.JPEG', 'ILSVRC2012_val_00022024.JPEG', 'ILSVRC2012_val_00011285.JPEG', 'ILSVRC2012_val_00017859.JPEG', 'ILSVRC2012_val_00025713.JPEG', 'ILSVRC2012_val_00027390.JPEG', 'ILSVRC2012_val_00045695.JPEG', 'ILSVRC2012_val_00038690.JPEG', 'ILSVRC2012_val_00016934.JPEG', 'ILSVRC2012_val_00027410.JPEG', 'ILSVRC2012_val_00039936.JPEG', 'ILSVRC2012_val_00025764.JPEG', 'ILSVRC2012_val_00024909.JPEG', 'ILSVRC2012_val_00003979.JPEG', 'ILSVRC2012_val_00035948.JPEG', 'ILSVRC2012_val_00044730.JPEG', 'ILSVRC2012_val_00041283.JPEG']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).eval().to(device)\n",
    "# target_layers = [model.features[10]]\n",
    "model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).eval()\n",
    "target_layers = [model.features[28]]\n",
    "\n",
    "cam_metric = utils.CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = utils.DropInConfidence()\n",
    "increase_in_conf_metric = utils.IncreaseInConfidence()\n",
    "\n",
    "# Process the randomly selected images\n",
    "for filename in random_image_names:\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    if os.path.exists(img_path):  # Ensure the file exists\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.float32(img) / 255\n",
    "        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_class = model(input_tensor).max(1)[-1]\n",
    "            targets = [ClassifierOutputTarget(predicted_class)]\n",
    "            print(predicted_class)\n",
    "            print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "        gray_scale = cam.GradCAMPlusPlus(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "        \n",
    "        threshold = np.percentile(gray_scale, 50)\n",
    "        gray_scale[gray_scale < threshold] = 0\n",
    "        \n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Calculate Drop in Confidence and Increase in Confidence\n",
    "        drop_in_conf = drop_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "        increase_in_conf = increase_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "\n",
    "        # Accumulate the scores for averaging\n",
    "        total_drop_in_conf += drop_in_conf\n",
    "        total_increase_in_conf += increase_in_conf\n",
    "        num_images += 1  # Increment image count\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        score = scores[0]\n",
    "        visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "\n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        # print(f\"Confidence before imputation: {scores_before}\")\n",
    "        # print(f\"Confidence after imputation: {scores_after}\")\n",
    "        # print(f\"The confidence increase raw: {score}\")\n",
    "        # print(f\"The drop in confidence percentage: {drop_in_conf}%\")\n",
    "        # print(f\"The increase in confidence: {increase_in_conf}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "\n",
    "# Calculate and print averages after processing all images\n",
    "if num_images > 0:\n",
    "    avg_drop_in_conf = total_drop_in_conf / num_images\n",
    "    avg_increase_in_conf = total_increase_in_conf / num_images * 100\n",
    "    print(f\"\\nNumber of images processed: {num_images}\")\n",
    "    print(f\"Average Drop in Confidence: {avg_drop_in_conf}%\")\n",
    "    print(f\"Average Increase in Confidence: {avg_increase_in_conf}%\")\n",
    "else:\n",
    "    print(\"No valid images found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Based Pointing Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../LOC_val_solution/LOC_val_solution.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../LOC_val_solution/LOC_val_solution.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your CSV file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m images_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../val_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the folder containing the images\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m, in \u001b[0;36mfilter_images\u001b[1;34m(csv_file, images_folder, num_samples)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_images\u001b[39m(csv_file, images_folder, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     29\u001b[0m     eligible_images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 30\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Shuffle data for randomness upfront\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[1;32mc:\\Users\\HaPham\\anaconda3\\envs\\score-cam-env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HaPham\\anaconda3\\envs\\score-cam-env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\HaPham\\anaconda3\\envs\\score-cam-env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HaPham\\anaconda3\\envs\\score-cam-env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\HaPham\\anaconda3\\envs\\score-cam-env\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../LOC_val_solution/LOC_val_solution.csv'"
     ]
    }
   ],
   "source": [
    "csv_file = \"../LOC_val_solution/LOC_val_solution.csv\"  # Replace with your CSV file\n",
    "images_folder = \"../val_folder\"  # Replace with the folder containing the images\n",
    "\n",
    "images = filter_images(csv_file=csv_file, images_folder=images_folder, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_proportion = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).eval().to(device)\n",
    "target_layers = [model.features[10]]\n",
    "\n",
    "for image in images:\n",
    "    img_path = f\"{images_folder}/{image['ImageId']}.JPEG\"\n",
    "\n",
    "    # Open image to get original dimensions\n",
    "    with Image.open(img_path) as img:\n",
    "        orig_width, orig_height = img.size\n",
    "\n",
    "    # Adjust bounding box to resized image\n",
    "    bbox = list(map(int, image[\"PredictionString\"].split()[1:5]))\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    x_min = int(x_min * 224 / orig_width)\n",
    "    y_min = int(y_min * 224 / orig_height)\n",
    "    x_max = int(x_max * 224 / orig_width)\n",
    "    y_max = int(y_max * 224 / orig_height)\n",
    "    resized_bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    # Run CAM visualization\n",
    "    img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "    gray_scale = cam.ScoreCAM(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "    # Draw resized bounding box\n",
    "    # draw_bb(img=img, bb=resized_bbox)\n",
    "\n",
    "    # Convert score CAM to tensor\n",
    "    score_cam_tensor = torch.from_numpy(gray_scale[0, :])\n",
    "\n",
    "    # Compute energy proportion\n",
    "    proportion = utils.energy_point_game(bbox=resized_bbox, saliency_map=score_cam_tensor)\n",
    "\n",
    "    total_proportion += proportion\n",
    "\n",
    "    print(proportion)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"Proportion:\", total_proportion / len(images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "score-cam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
