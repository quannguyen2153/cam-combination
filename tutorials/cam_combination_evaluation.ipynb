{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from typing import List\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_grad_cam as cam\n",
    "import pytorch_grad_cam.utils as utils\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, ClassifierOutputSoftmaxTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "\n",
    "def display_images_in_rows(rgb_img: np.ndarray, grayscale_list: List[torch.Tensor], labels: List[str]):\n",
    "    \"\"\"\n",
    "    Displays a list of images in rows, with each row containing up to max_images_per_row images.\n",
    "    \n",
    "    Args:\n",
    "        images (list of np.ndarray): List of images to display.\n",
    "        labels (list of str): List of labels for each image.\n",
    "        max_images_per_row (int): Maximum number of images per row (default is 4).\n",
    "    \"\"\"\n",
    "\n",
    "    images = [rgb_img]\n",
    "\n",
    "    for grayscale in grayscale_list:\n",
    "        images.append(show_cam_on_image(rgb_img, grayscale, use_rgb=True))\n",
    "\n",
    "    labels.insert(0, \"Image\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "    # Loop through the images and labels to display them\n",
    "    for ax, img, label in zip(axes, images, labels):\n",
    "        ax.imshow(img)  # Show the image\n",
    "        ax.set_title(label)  # Set the title\n",
    "        ax.axis('off')  # Hide axes\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "labels = json.load(open(\"../pytorch_grad_cam/utils/imagenet_class_index.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "input_folder = r\"../val_folder\"\n",
    "imputated_folder = \"../imputated_images\"\n",
    "\n",
    "if os.path.exists(imputated_folder):\n",
    "    shutil.rmtree(imputated_folder)\n",
    "os.makedirs(imputated_folder)\n",
    "\n",
    "# Generate a list of image names within the specified range\n",
    "start_idx = 1\n",
    "end_idx = 50000\n",
    "num_images_to_sample = 100  # Adjust this to how many random images you want\n",
    "\n",
    "all_image_names = [\n",
    "    f\"ILSVRC2012_val_{i:08d}.JPEG\" for i in range(start_idx, end_idx + 1)\n",
    "]\n",
    "random_image_names = random.sample(all_image_names, num_images_to_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_bb(img, bb):\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img, cmap='gray')\n",
    "\n",
    "    # Define the bounding box coordinates\n",
    "    x1, y1, x2, y2 = bb\n",
    "\n",
    "    # Calculate width and height\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    # Create a rectangle patch\n",
    "    rect = patches.Rectangle((x1, y1), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "\n",
    "    # Add the rectangle to the axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Show the image with the bounding box\n",
    "    plt.show()\n",
    "\n",
    "def filter_images(csv_file, images_folder, num_samples=None):\n",
    "    eligible_images = []\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data = data.sample(frac=1).reset_index(drop=True)  # Shuffle data for randomness upfront\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        image_id = row[\"ImageId\"]\n",
    "        predictions = row[\"PredictionString\"].split()\n",
    "\n",
    "        # Parse predictions into labels and coordinates\n",
    "        bboxes = []\n",
    "        for i in range(0, len(predictions), 5):\n",
    "            class_label = predictions[i]\n",
    "            coords = list(map(int, predictions[i + 1:i + 5]))\n",
    "            bboxes.append((class_label, coords))\n",
    "\n",
    "        # Skip if there are multiple bounding boxes\n",
    "        if len(bboxes) > 1:\n",
    "            continue\n",
    "\n",
    "        # Load the image to get dimensions\n",
    "        image_path = f\"{images_folder}/{image_id}.JPEG\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        # Calculate total bounding box area\n",
    "        total_bbox_area = sum(\n",
    "            (x_max - x_min) * (y_max - y_min)\n",
    "            for _, (x_min, y_min, x_max, y_max) in bboxes\n",
    "        )\n",
    "        image_area = img_width * img_height\n",
    "\n",
    "        # Skip if the object occupies 50% or more of the image\n",
    "        if total_bbox_area / image_area >= 0.5:\n",
    "            continue\n",
    "\n",
    "        # Add eligible image and bounding box to the list\n",
    "        eligible_images.append(row)\n",
    "\n",
    "        # Stop if we've collected enough samples\n",
    "        if num_samples and len(eligible_images) >= num_samples:\n",
    "            break\n",
    "\n",
    "    return eligible_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop in confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "total_drop_in_conf = 0\n",
    "total_increase_in_conf = 0\n",
    "num_images = 0  # Track number of processed images\n",
    "\n",
    "input_folder = r\"../val_folder\"\n",
    "random_image_names = ['ILSVRC2012_val_00048014.JPEG', 'ILSVRC2012_val_00040662.JPEG', 'ILSVRC2012_val_00021194.JPEG', 'ILSVRC2012_val_00038840.JPEG', 'ILSVRC2012_val_00026519.JPEG', 'ILSVRC2012_val_00026939.JPEG', 'ILSVRC2012_val_00000022.JPEG', 'ILSVRC2012_val_00000165.JPEG', 'ILSVRC2012_val_00028945.JPEG', 'ILSVRC2012_val_00032674.JPEG', 'ILSVRC2012_val_00026695.JPEG', 'ILSVRC2012_val_00043106.JPEG', 'ILSVRC2012_val_00038333.JPEG', 'ILSVRC2012_val_00018162.JPEG', 'ILSVRC2012_val_00004863.JPEG', 'ILSVRC2012_val_00034851.JPEG', 'ILSVRC2012_val_00004517.JPEG', 'ILSVRC2012_val_00010810.JPEG', 'ILSVRC2012_val_00011824.JPEG', 'ILSVRC2012_val_00020485.JPEG', 'ILSVRC2012_val_00048173.JPEG', 'ILSVRC2012_val_00041393.JPEG', 'ILSVRC2012_val_00033662.JPEG', 'ILSVRC2012_val_00045303.JPEG', 'ILSVRC2012_val_00004455.JPEG', 'ILSVRC2012_val_00011500.JPEG', 'ILSVRC2012_val_00025962.JPEG', 'ILSVRC2012_val_00043584.JPEG', 'ILSVRC2012_val_00036038.JPEG', 'ILSVRC2012_val_00001159.JPEG', 'ILSVRC2012_val_00036157.JPEG', 'ILSVRC2012_val_00047597.JPEG', 'ILSVRC2012_val_00044337.JPEG', 'ILSVRC2012_val_00003755.JPEG', 'ILSVRC2012_val_00043447.JPEG', 'ILSVRC2012_val_00031518.JPEG', 'ILSVRC2012_val_00041929.JPEG', 'ILSVRC2012_val_00010950.JPEG', 'ILSVRC2012_val_00023940.JPEG', 'ILSVRC2012_val_00034458.JPEG', 'ILSVRC2012_val_00003772.JPEG', 'ILSVRC2012_val_00017173.JPEG', 'ILSVRC2012_val_00035194.JPEG', 'ILSVRC2012_val_00013968.JPEG', 'ILSVRC2012_val_00007289.JPEG', 'ILSVRC2012_val_00035626.JPEG', 'ILSVRC2012_val_00001925.JPEG', 'ILSVRC2012_val_00018556.JPEG', 'ILSVRC2012_val_00005887.JPEG', 'ILSVRC2012_val_00037546.JPEG', 'ILSVRC2012_val_00037983.JPEG', 'ILSVRC2012_val_00028321.JPEG', 'ILSVRC2012_val_00006292.JPEG', 'ILSVRC2012_val_00010227.JPEG', 'ILSVRC2012_val_00020722.JPEG', 'ILSVRC2012_val_00010561.JPEG', 'ILSVRC2012_val_00040482.JPEG', 'ILSVRC2012_val_00042051.JPEG', 'ILSVRC2012_val_00001760.JPEG', 'ILSVRC2012_val_00021865.JPEG', 'ILSVRC2012_val_00010828.JPEG', 'ILSVRC2012_val_00043847.JPEG', 'ILSVRC2012_val_00036917.JPEG', 'ILSVRC2012_val_00047053.JPEG', 'ILSVRC2012_val_00002225.JPEG', 'ILSVRC2012_val_00014391.JPEG', 'ILSVRC2012_val_00023265.JPEG', 'ILSVRC2012_val_00025722.JPEG', 'ILSVRC2012_val_00035266.JPEG', 'ILSVRC2012_val_00000334.JPEG', 'ILSVRC2012_val_00009316.JPEG', 'ILSVRC2012_val_00037959.JPEG', 'ILSVRC2012_val_00015267.JPEG', 'ILSVRC2012_val_00045274.JPEG', 'ILSVRC2012_val_00005621.JPEG', 'ILSVRC2012_val_00009324.JPEG', 'ILSVRC2012_val_00036612.JPEG', 'ILSVRC2012_val_00012167.JPEG', 'ILSVRC2012_val_00013826.JPEG', 'ILSVRC2012_val_00039615.JPEG', 'ILSVRC2012_val_00003550.JPEG', 'ILSVRC2012_val_00018661.JPEG', 'ILSVRC2012_val_00037578.JPEG', 'ILSVRC2012_val_00032692.JPEG', 'ILSVRC2012_val_00022024.JPEG', 'ILSVRC2012_val_00011285.JPEG', 'ILSVRC2012_val_00017859.JPEG', 'ILSVRC2012_val_00025713.JPEG', 'ILSVRC2012_val_00027390.JPEG', 'ILSVRC2012_val_00045695.JPEG', 'ILSVRC2012_val_00038690.JPEG', 'ILSVRC2012_val_00016934.JPEG', 'ILSVRC2012_val_00027410.JPEG', 'ILSVRC2012_val_00039936.JPEG', 'ILSVRC2012_val_00025764.JPEG', 'ILSVRC2012_val_00024909.JPEG', 'ILSVRC2012_val_00003979.JPEG', 'ILSVRC2012_val_00035948.JPEG', 'ILSVRC2012_val_00044730.JPEG', 'ILSVRC2012_val_00041283.JPEG']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).eval().to(device)\n",
    "target_layers = [model.features[10]]\n",
    "\n",
    "cam_metric = utils.CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = utils.DropInConfidence()\n",
    "increase_in_conf_metric = utils.IncreaseInConfidence()\n",
    "\n",
    "# Process the randomly selected images\n",
    "for filename in random_image_names:\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    if os.path.exists(img_path):  # Ensure the file exists\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.float32(img) / 255\n",
    "        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_class = model(input_tensor).max(1)[-1]\n",
    "            targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "            print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "        grad_cam_pp_grayscale = cam.GradCAMPlusPlus(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "        score_cam_grayscale = cam.ScoreCAM(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "        gray_scale = utils.combine_by_matching_important_pixels(grayscales=[grad_cam_pp_grayscale[0, :], score_cam_grayscale[0, :]], thresholds=[25, 25])\n",
    "        gray_scale = np.expand_dims(gray_scale, axis=0)\n",
    "\n",
    "        threshold = np.percentile(gray_scale, 50)\n",
    "        gray_scale[gray_scale < threshold] = 0\n",
    "        \n",
    "        # display_images_in_rows(rgb_img=img, grayscale_list=[grad_cam_pp_grayscale[0, :], score_cam_grayscale[0, :], gray_scale[0, :]], labels=[\"GradCAM++\", \"ScoreCAM\", \"MatchingCAM\"])\n",
    "\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Calculate Drop in Confidence and Increase in Confidence\n",
    "        drop_in_conf = drop_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "        increase_in_conf = increase_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "\n",
    "        # Accumulate the scores for averaging\n",
    "        total_drop_in_conf += drop_in_conf\n",
    "        total_increase_in_conf += increase_in_conf\n",
    "        num_images += 1  # Increment image count\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        score = scores[0]\n",
    "        visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "\n",
    "        # Print individual results\n",
    "        print(f\"Image: {filename}\")\n",
    "        print(f\"Confidence before imputation: {scores_before}\")\n",
    "        print(f\"Confidence after imputation: {scores_after}\")\n",
    "        print(f\"The confidence increase raw: {score}\")\n",
    "        print(f\"The drop in confidence percentage: {drop_in_conf}%\")\n",
    "        print(f\"The increase in confidence: {increase_in_conf}\")\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "# Calculate and print averages after processing all images\n",
    "if num_images > 0:\n",
    "    avg_drop_in_conf = total_drop_in_conf / num_images\n",
    "    avg_increase_in_conf = total_increase_in_conf / num_images * 100\n",
    "    print(f\"\\nNumber of images processed: {num_images}\")\n",
    "    print(f\"Average Drop in Confidence: {avg_drop_in_conf}%\")\n",
    "    print(f\"Average Increase in Confidence: {avg_increase_in_conf}%\")\n",
    "else:\n",
    "    print(\"No valid images found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "total_drop_in_conf = 0\n",
    "total_increase_in_conf = 0\n",
    "num_images = 0  # Track number of processed images\n",
    "\n",
    "input_folder = r\"../val_folder\"\n",
    "random_image_names = ['ILSVRC2012_val_00048014.JPEG', 'ILSVRC2012_val_00040662.JPEG', 'ILSVRC2012_val_00021194.JPEG', 'ILSVRC2012_val_00038840.JPEG', 'ILSVRC2012_val_00026519.JPEG', 'ILSVRC2012_val_00026939.JPEG', 'ILSVRC2012_val_00000022.JPEG', 'ILSVRC2012_val_00000165.JPEG', 'ILSVRC2012_val_00028945.JPEG', 'ILSVRC2012_val_00032674.JPEG', 'ILSVRC2012_val_00026695.JPEG', 'ILSVRC2012_val_00043106.JPEG', 'ILSVRC2012_val_00038333.JPEG', 'ILSVRC2012_val_00018162.JPEG', 'ILSVRC2012_val_00004863.JPEG', 'ILSVRC2012_val_00034851.JPEG', 'ILSVRC2012_val_00004517.JPEG', 'ILSVRC2012_val_00010810.JPEG', 'ILSVRC2012_val_00011824.JPEG', 'ILSVRC2012_val_00020485.JPEG', 'ILSVRC2012_val_00048173.JPEG', 'ILSVRC2012_val_00041393.JPEG', 'ILSVRC2012_val_00033662.JPEG', 'ILSVRC2012_val_00045303.JPEG', 'ILSVRC2012_val_00004455.JPEG', 'ILSVRC2012_val_00011500.JPEG', 'ILSVRC2012_val_00025962.JPEG', 'ILSVRC2012_val_00043584.JPEG', 'ILSVRC2012_val_00036038.JPEG', 'ILSVRC2012_val_00001159.JPEG', 'ILSVRC2012_val_00036157.JPEG', 'ILSVRC2012_val_00047597.JPEG', 'ILSVRC2012_val_00044337.JPEG', 'ILSVRC2012_val_00003755.JPEG', 'ILSVRC2012_val_00043447.JPEG', 'ILSVRC2012_val_00031518.JPEG', 'ILSVRC2012_val_00041929.JPEG', 'ILSVRC2012_val_00010950.JPEG', 'ILSVRC2012_val_00023940.JPEG', 'ILSVRC2012_val_00034458.JPEG', 'ILSVRC2012_val_00003772.JPEG', 'ILSVRC2012_val_00017173.JPEG', 'ILSVRC2012_val_00035194.JPEG', 'ILSVRC2012_val_00013968.JPEG', 'ILSVRC2012_val_00007289.JPEG', 'ILSVRC2012_val_00035626.JPEG', 'ILSVRC2012_val_00001925.JPEG', 'ILSVRC2012_val_00018556.JPEG', 'ILSVRC2012_val_00005887.JPEG', 'ILSVRC2012_val_00037546.JPEG', 'ILSVRC2012_val_00037983.JPEG', 'ILSVRC2012_val_00028321.JPEG', 'ILSVRC2012_val_00006292.JPEG', 'ILSVRC2012_val_00010227.JPEG', 'ILSVRC2012_val_00020722.JPEG', 'ILSVRC2012_val_00010561.JPEG', 'ILSVRC2012_val_00040482.JPEG', 'ILSVRC2012_val_00042051.JPEG', 'ILSVRC2012_val_00001760.JPEG', 'ILSVRC2012_val_00021865.JPEG', 'ILSVRC2012_val_00010828.JPEG', 'ILSVRC2012_val_00043847.JPEG', 'ILSVRC2012_val_00036917.JPEG', 'ILSVRC2012_val_00047053.JPEG', 'ILSVRC2012_val_00002225.JPEG', 'ILSVRC2012_val_00014391.JPEG', 'ILSVRC2012_val_00023265.JPEG', 'ILSVRC2012_val_00025722.JPEG', 'ILSVRC2012_val_00035266.JPEG', 'ILSVRC2012_val_00000334.JPEG', 'ILSVRC2012_val_00009316.JPEG', 'ILSVRC2012_val_00037959.JPEG', 'ILSVRC2012_val_00015267.JPEG', 'ILSVRC2012_val_00045274.JPEG', 'ILSVRC2012_val_00005621.JPEG', 'ILSVRC2012_val_00009324.JPEG', 'ILSVRC2012_val_00036612.JPEG', 'ILSVRC2012_val_00012167.JPEG', 'ILSVRC2012_val_00013826.JPEG', 'ILSVRC2012_val_00039615.JPEG', 'ILSVRC2012_val_00003550.JPEG', 'ILSVRC2012_val_00018661.JPEG', 'ILSVRC2012_val_00037578.JPEG', 'ILSVRC2012_val_00032692.JPEG', 'ILSVRC2012_val_00022024.JPEG', 'ILSVRC2012_val_00011285.JPEG', 'ILSVRC2012_val_00017859.JPEG', 'ILSVRC2012_val_00025713.JPEG', 'ILSVRC2012_val_00027390.JPEG', 'ILSVRC2012_val_00045695.JPEG', 'ILSVRC2012_val_00038690.JPEG', 'ILSVRC2012_val_00016934.JPEG', 'ILSVRC2012_val_00027410.JPEG', 'ILSVRC2012_val_00039936.JPEG', 'ILSVRC2012_val_00025764.JPEG', 'ILSVRC2012_val_00024909.JPEG', 'ILSVRC2012_val_00003979.JPEG', 'ILSVRC2012_val_00035948.JPEG', 'ILSVRC2012_val_00044730.JPEG', 'ILSVRC2012_val_00041283.JPEG']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).eval().to(device)\n",
    "target_layers = [model.features[10]]\n",
    "\n",
    "cam_metric = utils.CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = utils.DropInConfidence()\n",
    "increase_in_conf_metric = utils.IncreaseInConfidence()\n",
    "\n",
    "# Process the randomly selected images\n",
    "for filename in random_image_names:\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    if os.path.exists(img_path):  # Ensure the file exists\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.float32(img) / 255\n",
    "        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_class = model(input_tensor).max(1)[-1]\n",
    "            targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "            print(predicted_class)\n",
    "            print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "        gray_scale = cam.GradCAM(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "        \n",
    "        threshold = np.percentile(gray_scale, 50)\n",
    "        gray_scale[gray_scale < threshold] = 0\n",
    "        \n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Calculate Drop in Confidence and Increase in Confidence\n",
    "        drop_in_conf = drop_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "        increase_in_conf = increase_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "\n",
    "        # Accumulate the scores for averaging\n",
    "        total_drop_in_conf += drop_in_conf\n",
    "        total_increase_in_conf += increase_in_conf\n",
    "        num_images += 1  # Increment image count\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        score = scores[0]\n",
    "        visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "\n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        # print(f\"Confidence before imputation: {scores_before}\")\n",
    "        # print(f\"Confidence after imputation: {scores_after}\")\n",
    "        # print(f\"The confidence increase raw: {score}\")\n",
    "        # print(f\"The drop in confidence percentage: {drop_in_conf}%\")\n",
    "        # print(f\"The increase in confidence: {increase_in_conf}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "\n",
    "# Calculate and print averages after processing all images\n",
    "if num_images > 0:\n",
    "    avg_drop_in_conf = total_drop_in_conf / num_images\n",
    "    avg_increase_in_conf = total_increase_in_conf / num_images * 100\n",
    "    print(f\"\\nNumber of images processed: {num_images}\")\n",
    "    print(f\"Average Drop in Confidence: {avg_drop_in_conf}%\")\n",
    "    print(f\"Average Increase in Confidence: {avg_increase_in_conf}%\")\n",
    "else:\n",
    "    print(\"No valid images found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "total_drop_in_conf = 0\n",
    "total_increase_in_conf = 0\n",
    "num_images = 0  # Track number of processed images\n",
    "\n",
    "input_folder = r\"../val_folder\"\n",
    "random_image_names = ['ILSVRC2012_val_00048014.JPEG', 'ILSVRC2012_val_00040662.JPEG', 'ILSVRC2012_val_00021194.JPEG', 'ILSVRC2012_val_00038840.JPEG', 'ILSVRC2012_val_00026519.JPEG', 'ILSVRC2012_val_00026939.JPEG', 'ILSVRC2012_val_00000022.JPEG', 'ILSVRC2012_val_00000165.JPEG', 'ILSVRC2012_val_00028945.JPEG', 'ILSVRC2012_val_00032674.JPEG', 'ILSVRC2012_val_00026695.JPEG', 'ILSVRC2012_val_00043106.JPEG', 'ILSVRC2012_val_00038333.JPEG', 'ILSVRC2012_val_00018162.JPEG', 'ILSVRC2012_val_00004863.JPEG', 'ILSVRC2012_val_00034851.JPEG', 'ILSVRC2012_val_00004517.JPEG', 'ILSVRC2012_val_00010810.JPEG', 'ILSVRC2012_val_00011824.JPEG', 'ILSVRC2012_val_00020485.JPEG', 'ILSVRC2012_val_00048173.JPEG', 'ILSVRC2012_val_00041393.JPEG', 'ILSVRC2012_val_00033662.JPEG', 'ILSVRC2012_val_00045303.JPEG', 'ILSVRC2012_val_00004455.JPEG', 'ILSVRC2012_val_00011500.JPEG', 'ILSVRC2012_val_00025962.JPEG', 'ILSVRC2012_val_00043584.JPEG', 'ILSVRC2012_val_00036038.JPEG', 'ILSVRC2012_val_00001159.JPEG', 'ILSVRC2012_val_00036157.JPEG', 'ILSVRC2012_val_00047597.JPEG', 'ILSVRC2012_val_00044337.JPEG', 'ILSVRC2012_val_00003755.JPEG', 'ILSVRC2012_val_00043447.JPEG', 'ILSVRC2012_val_00031518.JPEG', 'ILSVRC2012_val_00041929.JPEG', 'ILSVRC2012_val_00010950.JPEG', 'ILSVRC2012_val_00023940.JPEG', 'ILSVRC2012_val_00034458.JPEG', 'ILSVRC2012_val_00003772.JPEG', 'ILSVRC2012_val_00017173.JPEG', 'ILSVRC2012_val_00035194.JPEG', 'ILSVRC2012_val_00013968.JPEG', 'ILSVRC2012_val_00007289.JPEG', 'ILSVRC2012_val_00035626.JPEG', 'ILSVRC2012_val_00001925.JPEG', 'ILSVRC2012_val_00018556.JPEG', 'ILSVRC2012_val_00005887.JPEG', 'ILSVRC2012_val_00037546.JPEG', 'ILSVRC2012_val_00037983.JPEG', 'ILSVRC2012_val_00028321.JPEG', 'ILSVRC2012_val_00006292.JPEG', 'ILSVRC2012_val_00010227.JPEG', 'ILSVRC2012_val_00020722.JPEG', 'ILSVRC2012_val_00010561.JPEG', 'ILSVRC2012_val_00040482.JPEG', 'ILSVRC2012_val_00042051.JPEG', 'ILSVRC2012_val_00001760.JPEG', 'ILSVRC2012_val_00021865.JPEG', 'ILSVRC2012_val_00010828.JPEG', 'ILSVRC2012_val_00043847.JPEG', 'ILSVRC2012_val_00036917.JPEG', 'ILSVRC2012_val_00047053.JPEG', 'ILSVRC2012_val_00002225.JPEG', 'ILSVRC2012_val_00014391.JPEG', 'ILSVRC2012_val_00023265.JPEG', 'ILSVRC2012_val_00025722.JPEG', 'ILSVRC2012_val_00035266.JPEG', 'ILSVRC2012_val_00000334.JPEG', 'ILSVRC2012_val_00009316.JPEG', 'ILSVRC2012_val_00037959.JPEG', 'ILSVRC2012_val_00015267.JPEG', 'ILSVRC2012_val_00045274.JPEG', 'ILSVRC2012_val_00005621.JPEG', 'ILSVRC2012_val_00009324.JPEG', 'ILSVRC2012_val_00036612.JPEG', 'ILSVRC2012_val_00012167.JPEG', 'ILSVRC2012_val_00013826.JPEG', 'ILSVRC2012_val_00039615.JPEG', 'ILSVRC2012_val_00003550.JPEG', 'ILSVRC2012_val_00018661.JPEG', 'ILSVRC2012_val_00037578.JPEG', 'ILSVRC2012_val_00032692.JPEG', 'ILSVRC2012_val_00022024.JPEG', 'ILSVRC2012_val_00011285.JPEG', 'ILSVRC2012_val_00017859.JPEG', 'ILSVRC2012_val_00025713.JPEG', 'ILSVRC2012_val_00027390.JPEG', 'ILSVRC2012_val_00045695.JPEG', 'ILSVRC2012_val_00038690.JPEG', 'ILSVRC2012_val_00016934.JPEG', 'ILSVRC2012_val_00027410.JPEG', 'ILSVRC2012_val_00039936.JPEG', 'ILSVRC2012_val_00025764.JPEG', 'ILSVRC2012_val_00024909.JPEG', 'ILSVRC2012_val_00003979.JPEG', 'ILSVRC2012_val_00035948.JPEG', 'ILSVRC2012_val_00044730.JPEG', 'ILSVRC2012_val_00041283.JPEG']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).eval().to(device)\n",
    "target_layers = [model.features[10]]\n",
    "\n",
    "cam_metric = utils.CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = utils.DropInConfidence()\n",
    "increase_in_conf_metric = utils.IncreaseInConfidence()\n",
    "\n",
    "# Process the randomly selected images\n",
    "for filename in random_image_names:\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    if os.path.exists(img_path):  # Ensure the file exists\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.float32(img) / 255\n",
    "        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_class = model(input_tensor).max(1)[-1]\n",
    "            targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "            print(predicted_class)\n",
    "            print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "        gray_scale = cam.GradCAMPlusPlus(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "        \n",
    "        threshold = np.percentile(gray_scale, 50)\n",
    "        gray_scale[gray_scale < threshold] = 0\n",
    "        \n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Calculate Drop in Confidence and Increase in Confidence\n",
    "        drop_in_conf = drop_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "        increase_in_conf = increase_in_conf_metric(input_tensor, gray_scale, targets, model)\n",
    "\n",
    "        # Accumulate the scores for averaging\n",
    "        total_drop_in_conf += drop_in_conf\n",
    "        total_increase_in_conf += increase_in_conf\n",
    "        num_images += 1  # Increment image count\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        score = scores[0]\n",
    "        visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "\n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        # print(f\"Confidence before imputation: {scores_before}\")\n",
    "        # print(f\"Confidence after imputation: {scores_after}\")\n",
    "        # print(f\"The confidence increase raw: {score}\")\n",
    "        # print(f\"The drop in confidence percentage: {drop_in_conf}%\")\n",
    "        # print(f\"The increase in confidence: {increase_in_conf}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "\n",
    "# Calculate and print averages after processing all images\n",
    "if num_images > 0:\n",
    "    avg_drop_in_conf = total_drop_in_conf / num_images\n",
    "    avg_increase_in_conf = total_increase_in_conf / num_images * 100\n",
    "    print(f\"\\nNumber of images processed: {num_images}\")\n",
    "    print(f\"Average Drop in Confidence: {avg_drop_in_conf}%\")\n",
    "    print(f\"Average Increase in Confidence: {avg_increase_in_conf}%\")\n",
    "else:\n",
    "    print(\"No valid images found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Based Pointing Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"../LOC_val_solution/LOC_val_solution.csv\"  # Replace with your CSV file\n",
    "images_folder = \"../val_folder\"  # Replace with the folder containing the images\n",
    "\n",
    "images = filter_images(csv_file=csv_file, images_folder=images_folder, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_proportion = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).eval().to(device)\n",
    "target_layers = [model.features[10]]\n",
    "\n",
    "for image in images:\n",
    "    img_path = f\"{images_folder}/{image['ImageId']}.JPEG\"\n",
    "\n",
    "    # Open image to get original dimensions\n",
    "    with Image.open(img_path) as img:\n",
    "        orig_width, orig_height = img.size\n",
    "\n",
    "    # Adjust bounding box to resized image\n",
    "    bbox = list(map(int, image[\"PredictionString\"].split()[1:5]))\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    x_min = int(x_min * 224 / orig_width)\n",
    "    y_min = int(y_min * 224 / orig_height)\n",
    "    x_max = int(x_max * 224 / orig_width)\n",
    "    y_max = int(y_max * 224 / orig_height)\n",
    "    resized_bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    # Run CAM visualization\n",
    "    img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "    gray_scale = cam.ScoreCAM(model=model, target_layers=target_layers)(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "    # Draw resized bounding box\n",
    "    # draw_bb(img=img, bb=resized_bbox)\n",
    "\n",
    "    # Convert score CAM to tensor\n",
    "    score_cam_tensor = torch.from_numpy(gray_scale[0, :])\n",
    "\n",
    "    # Compute energy proportion\n",
    "    proportion = utils.energy_point_game(bbox=resized_bbox, saliency_map=score_cam_tensor)\n",
    "\n",
    "    total_proportion += proportion\n",
    "\n",
    "    print(proportion)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"Proportion:\", total_proportion / len(images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "score-cam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
