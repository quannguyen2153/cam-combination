{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from typing import Union, List\n",
    "from enum import Enum\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_grad_cam as cam\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, ClassifierOutputSoftmaxTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "class CAMType(Enum):\n",
    "    GRAD_CAM = cam.GradCAM\n",
    "    HI_RES_CAM = cam.HiResCAM\n",
    "    GRAD_CAM_ELEMENT_WISE = cam.GradCAMElementWise\n",
    "    ABLATION_CAM = cam.AblationCAM\n",
    "    X_GRAD_CAM = cam.XGradCAM\n",
    "    GRAD_CAM_PLUS_PLUS = cam.GradCAMPlusPlus\n",
    "    SCORE_CAM = cam.ScoreCAM\n",
    "    LAYER_CAM = cam.LayerCAM\n",
    "    EIGEN_CAM = cam.EigenCAM\n",
    "    EIGEN_GRAD_CAM = cam.EigenGradCAM\n",
    "    KPCA_CAM = cam.KPCA_CAM\n",
    "    RANDOM_CAM = cam.RandomCAM\n",
    "    FULL_GRAD = cam.FullGrad\n",
    "    GRAD_SCORE_CAM = cam.GradScoreCAM\n",
    "\n",
    "from typing import List, Callable\n",
    "import time\n",
    "\n",
    "def get_cam(cam_type: CAMType, model: torch.nn.Module, target_layers: torch.nn.Module):\n",
    "    cam_class = cam_type.value\n",
    "    return cam_class(model=model, target_layers=target_layers)\n",
    "\n",
    "def visualize(grayscale: torch.Tensor, rgb_img: np.ndarray):\n",
    "    visualization = show_cam_on_image(rgb_img, grayscale, use_rgb=True)\n",
    "    plt.imshow(visualization)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display_images_in_rows(rgb_img: np.ndarray, grayscale_list: List[torch.Tensor], labels: List[str]):\n",
    "    \"\"\"\n",
    "    Displays a list of images in rows, with each row containing up to max_images_per_row images.\n",
    "    \n",
    "    Args:\n",
    "        images (list of np.ndarray): List of images to display.\n",
    "        labels (list of str): List of labels for each image.\n",
    "        max_images_per_row (int): Maximum number of images per row (default is 4).\n",
    "    \"\"\"\n",
    "\n",
    "    images = [rgb_img]\n",
    "\n",
    "    for grayscale in grayscale_list:\n",
    "        images.append(show_cam_on_image(rgb_img, grayscale, use_rgb=True))\n",
    "\n",
    "    labels.insert(0, \"Image\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "    # Loop through the images and labels to display them\n",
    "    for ax, img, label in zip(axes, images, labels):\n",
    "        ax.imshow(img)  # Show the image\n",
    "        ax.set_title(label)  # Set the title\n",
    "        ax.axis('off')  # Hide axes\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def execute_cam(\n",
    "        img_path: str,\n",
    "        model: torch.nn.Module,\n",
    "        target_layers: List[torch.nn.Module],\n",
    "        cam_type: CAMType,\n",
    "        targets: Union[List[ClassifierOutputTarget], None] = None,\n",
    "        visualization: bool = False,\n",
    "        output_path: Union[str, None] = None,\n",
    "        timing: bool = False\n",
    "    ):\n",
    "        model_name = \"unknown\"\n",
    "        target_layer_name = \"unknown\"\n",
    "        try:\n",
    "            model_name = model._get_name()\n",
    "            target_layer_name = target_layers[0]._get_name()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(f\"Executing CAM on {model_name} with target layer {target_layer_name}:\")\n",
    "\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.float32(img) / 255\n",
    "        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        # Forward to get target class if not specified\n",
    "        if not targets:\n",
    "            with torch.no_grad():\n",
    "                predicted_class = model(input_tensor).max(1)[-1]\n",
    "                targets = [ClassifierOutputTarget(predicted_class)]\n",
    "                print(predicted_class)\n",
    "                print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "        cam = get_cam(cam_type=cam_type, model=model, target_layers=target_layers)\n",
    "\n",
    "        # Generate the CAM\n",
    "        if(timing):\n",
    "            start_time = time.time()\n",
    "            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "        else:\n",
    "            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "        # Overlay the CAM on the image\n",
    "        if visualization:\n",
    "            # Convert the PIL image to a NumPy array for visualization\n",
    "            rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0  # Normalize the pixel values\n",
    "            \n",
    "            visualize(grayscale=grayscale_cam[0, :], rgb_img=rgb_img)\n",
    "\n",
    "            # Optionally, you can save the resulting CAM visualization\n",
    "            if output_path:\n",
    "                Image.fromarray((visualization * 255).astype(np.uint8)).save(output_path)\n",
    "                print(f\"Saved output image to {output_path}\")\n",
    "        if(time):\n",
    "            return grayscale_cam, input_tensor, elapsed_time\n",
    "        else:\n",
    "            return grayscale_cam, input_tensor\n",
    "\n",
    "labels = json.load(open(\"../pytorch_grad_cam/utils/imagenet_class_index.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbationConfidenceMetric:\n",
    "    def __init__(self, perturbation):\n",
    "        self.perturbation = perturbation\n",
    "\n",
    "    def __call__(self, input_tensor: torch.Tensor,\n",
    "                 cams: np.ndarray,\n",
    "                 targets: List[Callable],\n",
    "                 model: torch.nn.Module,\n",
    "                 return_visualization=False,\n",
    "                 return_diff=True):\n",
    "\n",
    "        if return_diff:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)\n",
    "                scores = [target(output).cpu().numpy()\n",
    "                          for target, output in zip(targets, outputs)]\n",
    "                scores = np.float32(scores)\n",
    "\n",
    "        batch_size = input_tensor.size(0)\n",
    "        perturbated_tensors = []\n",
    "        for i in range(batch_size):\n",
    "            cam = cams[i]\n",
    "            tensor = self.perturbation(input_tensor[i, ...].cpu(),\n",
    "                                       torch.from_numpy(cam))\n",
    "            tensor = tensor.to(input_tensor.device)\n",
    "            perturbated_tensors.append(tensor.unsqueeze(0))\n",
    "        perturbated_tensors = torch.cat(perturbated_tensors)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_after_imputation = model(perturbated_tensors)\n",
    "        scores_after_imputation = [\n",
    "            target(output).cpu().numpy() for target, output in zip(\n",
    "                targets, outputs_after_imputation)]\n",
    "        scores_after_imputation = np.float32(scores_after_imputation)\n",
    "\n",
    "        if return_diff:\n",
    "            result = scores_after_imputation - scores\n",
    "        else:\n",
    "            result = scores_after_imputation\n",
    "\n",
    "        if return_visualization:\n",
    "            return result, scores, scores_after_imputation, perturbated_tensors\n",
    "        else:\n",
    "            return result, scores, scores_after_imputation\n",
    "\n",
    "def multiply_tensor_with_cam(input_tensor: torch.Tensor,\n",
    "                             cam: torch.Tensor):\n",
    "    \"\"\" Multiply an input tensor (after normalization)\n",
    "        with a pixel attribution map\n",
    "    \"\"\"\n",
    "    return input_tensor * cam\n",
    "        \n",
    "class CamMultImageConfidenceChange(PerturbationConfidenceMetric):\n",
    "    def __init__(self):\n",
    "        super(CamMultImageConfidenceChange,\n",
    "              self).__init__(multiply_tensor_with_cam)\n",
    "        \n",
    "class DropInConfidence(CamMultImageConfidenceChange):\n",
    "    def __init__(self):\n",
    "        super(DropInConfidence, self).__init__()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        scores, scores_before, scores_after = super(DropInConfidence, self).__call__(*args, **kwargs)\n",
    "        scores = -scores\n",
    "        return np.maximum(scores, 0) / scores_before * 100\n",
    "\n",
    "\n",
    "class IncreaseInConfidence(CamMultImageConfidenceChange):\n",
    "    def __init__(self):\n",
    "        super(IncreaseInConfidence, self).__init__()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        scores, bef_score, scores_after = super(IncreaseInConfidence, self).__call__(*args, **kwargs)\n",
    "        return np.float32(scores > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model & method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).eval()\n",
    "target_layers = [model.features[28]]\n",
    "\n",
    "# model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).eval()\n",
    "# target_layers = [model.features[10]]\n",
    "\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed decrease rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "input_folder = r\"C:\\Users\\HaPham\\Documents\\ThesisXAI\\Code\\CAM-combination\\ILSVRC2012\\ILSVRC2012_img_val\"\n",
    "imputated_folder = \"../imputated_images\"\n",
    "\n",
    "if os.path.exists(imputated_folder):\n",
    "    shutil.rmtree(imputated_folder)\n",
    "os.makedirs(imputated_folder)\n",
    "\n",
    "# Generate a list of image names within the specified range\n",
    "start_idx = 1\n",
    "end_idx = 50000\n",
    "num_images_to_sample = 10  # Adjust this to how many random images you want\n",
    "\n",
    "all_image_names = [\n",
    "    f\"ILSVRC2012_val_{i:08d}.JPEG\" for i in range(start_idx, end_idx + 1)\n",
    "]\n",
    "random_image_names = random.sample(all_image_names, num_images_to_sample)\n",
    "# random_image_names = ['ILSVRC2012_val_00048014.JPEG', 'ILSVRC2012_val_00040662.JPEG', 'ILSVRC2012_val_00021194.JPEG', 'ILSVRC2012_val_00038840.JPEG', 'ILSVRC2012_val_00026519.JPEG', 'ILSVRC2012_val_00026939.JPEG', 'ILSVRC2012_val_00000022.JPEG', 'ILSVRC2012_val_00000165.JPEG', 'ILSVRC2012_val_00028945.JPEG', 'ILSVRC2012_val_00032674.JPEG', 'ILSVRC2012_val_00026695.JPEG', 'ILSVRC2012_val_00043106.JPEG', 'ILSVRC2012_val_00038333.JPEG', 'ILSVRC2012_val_00018162.JPEG', 'ILSVRC2012_val_00004863.JPEG', 'ILSVRC2012_val_00034851.JPEG', 'ILSVRC2012_val_00004517.JPEG', 'ILSVRC2012_val_00010810.JPEG', 'ILSVRC2012_val_00011824.JPEG', 'ILSVRC2012_val_00020485.JPEG', 'ILSVRC2012_val_00048173.JPEG', 'ILSVRC2012_val_00041393.JPEG', 'ILSVRC2012_val_00033662.JPEG', 'ILSVRC2012_val_00045303.JPEG', 'ILSVRC2012_val_00004455.JPEG', 'ILSVRC2012_val_00011500.JPEG', 'ILSVRC2012_val_00025962.JPEG', 'ILSVRC2012_val_00043584.JPEG', 'ILSVRC2012_val_00036038.JPEG', 'ILSVRC2012_val_00001159.JPEG', 'ILSVRC2012_val_00036157.JPEG', 'ILSVRC2012_val_00047597.JPEG', 'ILSVRC2012_val_00044337.JPEG', 'ILSVRC2012_val_00003755.JPEG', 'ILSVRC2012_val_00043447.JPEG', 'ILSVRC2012_val_00031518.JPEG', 'ILSVRC2012_val_00041929.JPEG', 'ILSVRC2012_val_00010950.JPEG', 'ILSVRC2012_val_00023940.JPEG', 'ILSVRC2012_val_00034458.JPEG', 'ILSVRC2012_val_00003772.JPEG', 'ILSVRC2012_val_00017173.JPEG', 'ILSVRC2012_val_00035194.JPEG', 'ILSVRC2012_val_00013968.JPEG', 'ILSVRC2012_val_00007289.JPEG', 'ILSVRC2012_val_00035626.JPEG', 'ILSVRC2012_val_00001925.JPEG', 'ILSVRC2012_val_00018556.JPEG', 'ILSVRC2012_val_00005887.JPEG', 'ILSVRC2012_val_00037546.JPEG', 'ILSVRC2012_val_00037983.JPEG', 'ILSVRC2012_val_00028321.JPEG', 'ILSVRC2012_val_00006292.JPEG', 'ILSVRC2012_val_00010227.JPEG', 'ILSVRC2012_val_00020722.JPEG', 'ILSVRC2012_val_00010561.JPEG', 'ILSVRC2012_val_00040482.JPEG', 'ILSVRC2012_val_00042051.JPEG', 'ILSVRC2012_val_00001760.JPEG', 'ILSVRC2012_val_00021865.JPEG', 'ILSVRC2012_val_00010828.JPEG', 'ILSVRC2012_val_00043847.JPEG', 'ILSVRC2012_val_00036917.JPEG', 'ILSVRC2012_val_00047053.JPEG', 'ILSVRC2012_val_00002225.JPEG', 'ILSVRC2012_val_00014391.JPEG', 'ILSVRC2012_val_00023265.JPEG', 'ILSVRC2012_val_00025722.JPEG', 'ILSVRC2012_val_00035266.JPEG', 'ILSVRC2012_val_00000334.JPEG', 'ILSVRC2012_val_00009316.JPEG', 'ILSVRC2012_val_00037959.JPEG', 'ILSVRC2012_val_00015267.JPEG', 'ILSVRC2012_val_00045274.JPEG', 'ILSVRC2012_val_00005621.JPEG', 'ILSVRC2012_val_00009324.JPEG', 'ILSVRC2012_val_00036612.JPEG', 'ILSVRC2012_val_00012167.JPEG', 'ILSVRC2012_val_00013826.JPEG', 'ILSVRC2012_val_00039615.JPEG', 'ILSVRC2012_val_00003550.JPEG', 'ILSVRC2012_val_00018661.JPEG', 'ILSVRC2012_val_00037578.JPEG', 'ILSVRC2012_val_00032692.JPEG', 'ILSVRC2012_val_00022024.JPEG', 'ILSVRC2012_val_00011285.JPEG', 'ILSVRC2012_val_00017859.JPEG', 'ILSVRC2012_val_00025713.JPEG', 'ILSVRC2012_val_00027390.JPEG', 'ILSVRC2012_val_00045695.JPEG', 'ILSVRC2012_val_00038690.JPEG', 'ILSVRC2012_val_00016934.JPEG', 'ILSVRC2012_val_00027410.JPEG', 'ILSVRC2012_val_00039936.JPEG', 'ILSVRC2012_val_00025764.JPEG', 'ILSVRC2012_val_00024909.JPEG', 'ILSVRC2012_val_00003979.JPEG', 'ILSVRC2012_val_00035948.JPEG', 'ILSVRC2012_val_00044730.JPEG', 'ILSVRC2012_val_00041283.JPEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([852])\n",
      "Target class: tennis_ball\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:14<00:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: ILSVRC2012_val_00000653.JPEG\n",
      "Time: 74.4090313911438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics\n",
    "num_images = 0  # Track number of processed images\n",
    "\n",
    "# Process the randomly selected images\n",
    "for filename in random_image_names:\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    if os.path.exists(img_path):  # Ensure the file exists\n",
    "        # Execute CAM for the current image\n",
    "        gray_scale, input_tensor, calc_time_score = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=CAMType.SCORE_CAM, timing=True)\n",
    "        gray_scale, input_tensor, calc_time_grad_score = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=CAMType.GRAD_SCORE_CAM, timing=True)\n",
    "        print(f\"Image: {filename}\")\n",
    "        print(f\"calc_time_score: {calc_time_score}\")\n",
    "        print(f\"calc_time_grad_score: {calc_time_grad_score}\")\n",
    "        print(f\"Speed decrease rate: {(calc_time_score - calc_time_grad_score) / calc_time_score * 100}\")\n",
    "        print(\"-------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "score-cam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
