{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from typing import Union, List\n",
    "from enum import Enum\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_grad_cam as cam\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, ClassifierOutputSoftmaxTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "class CAMType(Enum):\n",
    "    GRAD_CAM = cam.GradCAM\n",
    "    HI_RES_CAM = cam.HiResCAM\n",
    "    GRAD_CAM_ELEMENT_WISE = cam.GradCAMElementWise\n",
    "    ABLATION_CAM = cam.AblationCAM\n",
    "    X_GRAD_CAM = cam.XGradCAM\n",
    "    GRAD_CAM_PLUS_PLUS = cam.GradCAMPlusPlus\n",
    "    SCORE_CAM = cam.ScoreCAM\n",
    "    LAYER_CAM = cam.LayerCAM\n",
    "    EIGEN_CAM = cam.EigenCAM\n",
    "    EIGEN_GRAD_CAM = cam.EigenGradCAM\n",
    "    KPCA_CAM = cam.KPCA_CAM\n",
    "    RANDOM_CAM = cam.RandomCAM\n",
    "    FULL_GRAD = cam.FullGrad\n",
    "\n",
    "from typing import List, Callable\n",
    "\n",
    "def get_cam(cam_type: CAMType, model: torch.nn.Module, target_layers: torch.nn.Module):\n",
    "    cam_class = cam_type.value\n",
    "    return cam_class(model=model, target_layers=target_layers)\n",
    "\n",
    "def visualize(grayscale: torch.Tensor, rgb_img: np.ndarray):\n",
    "    visualization = show_cam_on_image(rgb_img, grayscale, use_rgb=True)\n",
    "    plt.imshow(visualization)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display_images_in_rows(rgb_img: np.ndarray, grayscale_list: List[torch.Tensor], labels: List[str]):\n",
    "    \"\"\"\n",
    "    Displays a list of images in rows, with each row containing up to max_images_per_row images.\n",
    "    \n",
    "    Args:\n",
    "        images (list of np.ndarray): List of images to display.\n",
    "        labels (list of str): List of labels for each image.\n",
    "        max_images_per_row (int): Maximum number of images per row (default is 4).\n",
    "    \"\"\"\n",
    "\n",
    "    images = [rgb_img]\n",
    "\n",
    "    for grayscale in grayscale_list:\n",
    "        images.append(show_cam_on_image(rgb_img, grayscale, use_rgb=True))\n",
    "\n",
    "    labels.insert(0, \"Image\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "    # Loop through the images and labels to display them\n",
    "    for ax, img, label in zip(axes, images, labels):\n",
    "        ax.imshow(img)  # Show the image\n",
    "        ax.set_title(label)  # Set the title\n",
    "        ax.axis('off')  # Hide axes\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def execute_cam(\n",
    "        img_path: str,\n",
    "        model: torch.nn.Module,\n",
    "        target_layers: List[torch.nn.Module],\n",
    "        cam_type: CAMType,\n",
    "        targets: Union[List[ClassifierOutputTarget], None] = None,\n",
    "        visualization: bool = False,\n",
    "        output_path: Union[str, None] = None\n",
    "    ):\n",
    "        model_name = \"unknown\"\n",
    "        target_layer_name = \"unknown\"\n",
    "        try:\n",
    "            model_name = model._get_name()\n",
    "            target_layer_name = target_layers[0]._get_name()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(f\"Executing CAM on {model_name} with target layer {target_layer_name}:\")\n",
    "\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.float32(img) / 255\n",
    "        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        # Forward to get target class if not specified\n",
    "        if not targets:\n",
    "            with torch.no_grad():\n",
    "                predicted_class = model(input_tensor).max(1)[-1]\n",
    "                targets = [ClassifierOutputTarget(predicted_class)]\n",
    "                print(predicted_class)\n",
    "                print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "        cam = get_cam(cam_type=cam_type, model=model, target_layers=target_layers)\n",
    "\n",
    "        # Generate the CAM\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "        # Overlay the CAM on the image\n",
    "        if visualization:\n",
    "            # Convert the PIL image to a NumPy array for visualization\n",
    "            rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0  # Normalize the pixel values\n",
    "            \n",
    "            visualize(grayscale=grayscale_cam[0, :], rgb_img=rgb_img)\n",
    "\n",
    "            # Optionally, you can save the resulting CAM visualization\n",
    "            if output_path:\n",
    "                Image.fromarray((visualization * 255).astype(np.uint8)).save(output_path)\n",
    "                print(f\"Saved output image to {output_path}\")\n",
    "\n",
    "        return grayscale_cam, input_tensor\n",
    "\n",
    "labels = json.load(open(\"../pytorch_grad_cam/utils/imagenet_class_index.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbationConfidenceMetric:\n",
    "    def __init__(self, perturbation):\n",
    "        self.perturbation = perturbation\n",
    "\n",
    "    def __call__(self, input_tensor: torch.Tensor,\n",
    "                 cams: np.ndarray,\n",
    "                 targets: List[Callable],\n",
    "                 model: torch.nn.Module,\n",
    "                 return_visualization=False,\n",
    "                 return_diff=True):\n",
    "\n",
    "        if return_diff:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)\n",
    "                scores = [target(output).cpu().numpy()\n",
    "                          for target, output in zip(targets, outputs)]\n",
    "                scores = np.float32(scores)\n",
    "\n",
    "        batch_size = input_tensor.size(0)\n",
    "        perturbated_tensors = []\n",
    "        for i in range(batch_size):\n",
    "            cam = cams[i]\n",
    "            tensor = self.perturbation(input_tensor[i, ...].cpu(),\n",
    "                                       torch.from_numpy(cam))\n",
    "            tensor = tensor.to(input_tensor.device)\n",
    "            perturbated_tensors.append(tensor.unsqueeze(0))\n",
    "        perturbated_tensors = torch.cat(perturbated_tensors)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_after_imputation = model(perturbated_tensors)\n",
    "        scores_after_imputation = [\n",
    "            target(output).cpu().numpy() for target, output in zip(\n",
    "                targets, outputs_after_imputation)]\n",
    "        scores_after_imputation = np.float32(scores_after_imputation)\n",
    "\n",
    "        if return_diff:\n",
    "            result = scores_after_imputation - scores\n",
    "        else:\n",
    "            result = scores_after_imputation\n",
    "\n",
    "        if return_visualization:\n",
    "            return result, scores, scores_after_imputation, perturbated_tensors\n",
    "        else:\n",
    "            return result, scores, scores_after_imputation\n",
    "\n",
    "def multiply_tensor_with_cam(input_tensor: torch.Tensor,\n",
    "                             cam: torch.Tensor):\n",
    "    \"\"\" Multiply an input tensor (after normalization)\n",
    "        with a pixel attribution map\n",
    "    \"\"\"\n",
    "    return input_tensor * cam\n",
    "        \n",
    "class CamMultImageConfidenceChange(PerturbationConfidenceMetric):\n",
    "    def __init__(self):\n",
    "        super(CamMultImageConfidenceChange,\n",
    "              self).__init__(multiply_tensor_with_cam)\n",
    "        \n",
    "class DropInConfidence(CamMultImageConfidenceChange):\n",
    "    def __init__(self):\n",
    "        super(DropInConfidence, self).__init__()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        scores, scores_before, scores_after = super(DropInConfidence, self).__call__(*args, **kwargs)\n",
    "        scores = -scores\n",
    "        return np.maximum(scores, 0) / scores_before * 100\n",
    "\n",
    "\n",
    "class IncreaseInConfidence(CamMultImageConfidenceChange):\n",
    "    def __init__(self):\n",
    "        super(IncreaseInConfidence, self).__init__()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        scores, bef_score, scores_after = super(IncreaseInConfidence, self).__call__(*args, **kwargs)\n",
    "        return np.float32(scores > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model & method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).eval()\n",
    "target_layers = [model.features[28]]\n",
    "\n",
    "cam_type = CAMType.GRAD_CAM_PLUS_PLUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence of image 0% pixel and 100% pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "input_folder = r\"C:\\Users\\HaPham\\Documents\\ThesisXAI\\Code\\CAM-combination\\ILSVRC2012\\ILSVRC2012_img_val\"\n",
    "imputated_folder = \"../imputated_images\"\n",
    "\n",
    "if os.path.exists(imputated_folder):\n",
    "    shutil.rmtree(imputated_folder)\n",
    "os.makedirs(imputated_folder)\n",
    "\n",
    "# Generate a list of image names within the specified range\n",
    "start_idx = 1\n",
    "end_idx = 50000\n",
    "num_images_to_sample = 1  # Adjust this to how many random images you want\n",
    "\n",
    "all_image_names = [\n",
    "    f\"ILSVRC2012_val_{i:08d}.JPEG\" for i in range(start_idx, end_idx + 1)\n",
    "]\n",
    "random_image_names = random.sample(all_image_names, num_images_to_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([22])\n",
      "Target class: bald_eagle\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Confidence 100% img: [[0.999977]]\n",
      "Confidence 0% img: [[0.00022715]]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images\"\n",
    "for filename in random_image_names:\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    gray_scale = np.zeros_like(gray_scale_original)\n",
    "\n",
    "    # Load and preprocess the image for CAM overlay\n",
    "    rgb_img = Image.open(img_path).convert('RGB')\n",
    "    rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "    img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "    # Calculate predicted class and confidence score change\n",
    "    predicted_class = model(input_tensor).max(1)[-1]\n",
    "    # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "    targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "    scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "        input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "    )\n",
    "\n",
    "    # Process the visualization for display and scoring\n",
    "    score = scores[0]\n",
    "    visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "    visualization = deprocess_image(visualization)\n",
    "\n",
    "    # Save the visualization\n",
    "    # Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "    # Print individual results\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"Confidence 100% img: {scores_before}\")\n",
    "    print(f\"Confidence 0% img: {scores_after}\")\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([22])\n",
      "Target class: bald_eagle\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 100\tConfidence: [[0.00022715]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 99\tConfidence: [[0.01567769]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 98\tConfidence: [[0.00345962]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 97\tConfidence: [[0.00307689]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 96\tConfidence: [[0.00065779]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 95\tConfidence: [[0.03986163]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 94\tConfidence: [[0.15234204]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 93\tConfidence: [[0.0494281]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 92\tConfidence: [[0.06362353]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 91\tConfidence: [[0.05731063]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 90\tConfidence: [[0.07917179]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 89\tConfidence: [[0.05182704]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 88\tConfidence: [[0.07590158]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 87\tConfidence: [[0.02774854]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 86\tConfidence: [[0.0209269]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 85\tConfidence: [[0.01280403]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 84\tConfidence: [[0.01117317]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 83\tConfidence: [[0.07704518]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 82\tConfidence: [[0.23025438]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 81\tConfidence: [[0.48748332]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 80\tConfidence: [[0.6025816]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 79\tConfidence: [[0.6162823]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 78\tConfidence: [[0.7296782]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 77\tConfidence: [[0.8216668]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 76\tConfidence: [[0.80250037]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 75\tConfidence: [[0.8889357]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 74\tConfidence: [[0.9235159]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 73\tConfidence: [[0.96616715]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 72\tConfidence: [[0.9699014]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 71\tConfidence: [[0.96221936]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 70\tConfidence: [[0.97095495]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 69\tConfidence: [[0.9804544]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 68\tConfidence: [[0.98570466]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 67\tConfidence: [[0.99632394]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 66\tConfidence: [[0.99639136]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 65\tConfidence: [[0.99850106]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 64\tConfidence: [[0.9978789]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 63\tConfidence: [[0.9984744]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 62\tConfidence: [[0.99846566]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 61\tConfidence: [[0.9988317]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 60\tConfidence: [[0.9986094]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 59\tConfidence: [[0.99880314]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 58\tConfidence: [[0.99880433]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 57\tConfidence: [[0.99916553]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 56\tConfidence: [[0.9993832]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 55\tConfidence: [[0.9993075]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 54\tConfidence: [[0.9996606]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 53\tConfidence: [[0.99982303]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 52\tConfidence: [[0.99966097]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 51\tConfidence: [[0.9997875]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 50\tConfidence: [[0.99988294]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 49\tConfidence: [[0.9998696]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 48\tConfidence: [[0.999933]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 47\tConfidence: [[0.999951]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 46\tConfidence: [[0.9999312]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 45\tConfidence: [[0.9999082]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 44\tConfidence: [[0.9999075]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 43\tConfidence: [[0.9999069]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 42\tConfidence: [[0.99990785]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 41\tConfidence: [[0.9998456]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 40\tConfidence: [[0.99986625]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 39\tConfidence: [[0.99983954]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 38\tConfidence: [[0.9997869]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 37\tConfidence: [[0.9998227]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 36\tConfidence: [[0.99982625]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 35\tConfidence: [[0.99982905]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 34\tConfidence: [[0.9998636]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 33\tConfidence: [[0.99989045]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 32\tConfidence: [[0.99984825]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 31\tConfidence: [[0.9998497]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 30\tConfidence: [[0.99982446]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 29\tConfidence: [[0.99987435]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 28\tConfidence: [[0.9998691]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 27\tConfidence: [[0.9998965]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 26\tConfidence: [[0.99992776]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 25\tConfidence: [[0.9999672]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 24\tConfidence: [[0.99997795]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 23\tConfidence: [[0.9999678]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 22\tConfidence: [[0.9999465]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 21\tConfidence: [[0.9999645]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 20\tConfidence: [[0.99997485]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 19\tConfidence: [[0.9999685]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 18\tConfidence: [[0.9999629]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 17\tConfidence: [[0.999969]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 16\tConfidence: [[0.99995565]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 15\tConfidence: [[0.99996483]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 14\tConfidence: [[0.9999641]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 13\tConfidence: [[0.99996006]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 12\tConfidence: [[0.9999676]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 11\tConfidence: [[0.9999651]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 10\tConfidence: [[0.9999752]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 9\tConfidence: [[0.9999764]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 8\tConfidence: [[0.9999745]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 7\tConfidence: [[0.99997866]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 6\tConfidence: [[0.9999771]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 5\tConfidence: [[0.99996674]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 4\tConfidence: [[0.9999542]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 3\tConfidence: [[0.9999287]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 2\tConfidence: [[0.99993193]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 1\tConfidence: [[0.9999341]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 0\tConfidence: [[0.999977]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "AUC: 0.8015144599194173\n",
      "Average AUC: 0.8015144599194173\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images_insertion\"\n",
    "\n",
    "count = 0\n",
    "sum_auc = 0\n",
    "for filename in random_image_names:\n",
    "    confidences = []\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    for i in range(100,-1,-1):\n",
    "        if(i == 100):\n",
    "            gray_scale = np.zeros_like(gray_scale_original)\n",
    "        elif(i == 0):\n",
    "            gray_scale = np.ones_like(gray_scale_original)\n",
    "        else:\n",
    "            gray_scale = gray_scale_original.copy()\n",
    "            threshold = np.percentile(gray_scale, i)\n",
    "            gray_scale = gray_scale >= threshold\n",
    "\n",
    "        # Load and preprocess the image for CAM overlay\n",
    "        rgb_img = Image.open(img_path).convert('RGB')\n",
    "        rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "        img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "        # Calculate predicted class and confidence score change\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        # score = scores[0]\n",
    "        # visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        # visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, f\"{os.path.splitext(filename)[0]}_{i}{os.path.splitext(filename)[1]}\"))\n",
    "        confidences.append(scores_after)\n",
    "        \n",
    "        # Print individual results\n",
    "        print(f\"Image: {filename}\")\n",
    "        print(f\"Step: {i}\\tConfidence: {scores_after}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "    confidences = np.array(confidences).flatten()\n",
    "    confidences = (confidences - confidences.min()) / (confidences.max() - confidences.min())\n",
    "    fraction_revealed = np.linspace(0, 1, len(confidences))\n",
    "    auc = np.trapz(confidences, fraction_revealed)\n",
    "    count += 1\n",
    "    sum_auc += auc\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "print(f\"Average AUC: {sum_auc/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([22])\n",
      "Target class: bald_eagle\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 100\tConfidence: [[0.999977]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 99\tConfidence: [[0.99994314]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 98\tConfidence: [[0.99993086]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 97\tConfidence: [[0.9998547]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 96\tConfidence: [[0.999749]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 95\tConfidence: [[0.9998429]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 94\tConfidence: [[0.9999093]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 93\tConfidence: [[0.9997695]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 92\tConfidence: [[0.99951637]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 91\tConfidence: [[0.9983398]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 90\tConfidence: [[0.99423015]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 89\tConfidence: [[0.9872519]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 88\tConfidence: [[0.95198745]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 87\tConfidence: [[0.9327129]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 86\tConfidence: [[0.8994199]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 85\tConfidence: [[0.8564693]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 84\tConfidence: [[0.89453316]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 83\tConfidence: [[0.8615376]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 82\tConfidence: [[0.8047186]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 81\tConfidence: [[0.8110876]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 80\tConfidence: [[0.86340123]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 79\tConfidence: [[0.8952551]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 78\tConfidence: [[0.9048773]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 77\tConfidence: [[0.88564193]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 76\tConfidence: [[0.891917]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 75\tConfidence: [[0.86768]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 74\tConfidence: [[0.81640875]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 73\tConfidence: [[0.8020902]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 72\tConfidence: [[0.74891347]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 71\tConfidence: [[0.71000856]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 70\tConfidence: [[0.44600964]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 69\tConfidence: [[0.38750234]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 68\tConfidence: [[0.4619016]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 67\tConfidence: [[0.24422358]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 66\tConfidence: [[0.32149056]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 65\tConfidence: [[0.2520646]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 64\tConfidence: [[0.20259334]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 63\tConfidence: [[0.3423831]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 62\tConfidence: [[0.39269078]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 61\tConfidence: [[0.34283602]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 60\tConfidence: [[0.43385562]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 59\tConfidence: [[0.346859]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 58\tConfidence: [[0.20255949]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 57\tConfidence: [[0.16851741]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 56\tConfidence: [[0.1351773]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 55\tConfidence: [[0.07155248]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 54\tConfidence: [[0.07028056]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 53\tConfidence: [[0.04865432]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 52\tConfidence: [[0.03219535]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 51\tConfidence: [[0.03230597]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 50\tConfidence: [[0.0233466]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 49\tConfidence: [[0.03841076]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 48\tConfidence: [[0.05929985]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 47\tConfidence: [[0.09167371]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 46\tConfidence: [[0.10196019]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 45\tConfidence: [[0.03889269]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 44\tConfidence: [[0.04985374]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 43\tConfidence: [[0.04164812]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 42\tConfidence: [[0.01891412]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 41\tConfidence: [[0.01549918]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 40\tConfidence: [[0.00588769]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 39\tConfidence: [[0.00545484]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 38\tConfidence: [[0.00218813]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 37\tConfidence: [[0.00298288]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 36\tConfidence: [[0.00292276]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 35\tConfidence: [[0.001965]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 34\tConfidence: [[0.00230555]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 33\tConfidence: [[0.00126991]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 32\tConfidence: [[0.00312193]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 31\tConfidence: [[0.00442167]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 30\tConfidence: [[0.00620421]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 29\tConfidence: [[0.00351602]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 28\tConfidence: [[0.00980052]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 27\tConfidence: [[0.00765843]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 26\tConfidence: [[0.00575871]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 25\tConfidence: [[0.00172014]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 24\tConfidence: [[0.00301985]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 23\tConfidence: [[0.0011863]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 22\tConfidence: [[0.00172763]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 21\tConfidence: [[0.00068655]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 20\tConfidence: [[0.00067264]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 19\tConfidence: [[0.00047235]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 18\tConfidence: [[0.00030499]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 17\tConfidence: [[0.00032726]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 16\tConfidence: [[0.00016182]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 15\tConfidence: [[0.0002367]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 14\tConfidence: [[0.00011209]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 13\tConfidence: [[9.694354e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 12\tConfidence: [[4.053009e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 11\tConfidence: [[2.2133014e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 10\tConfidence: [[2.1626973e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 9\tConfidence: [[1.1669312e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 8\tConfidence: [[9.410384e-06]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 7\tConfidence: [[1.12568305e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 6\tConfidence: [[3.1335796e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 5\tConfidence: [[3.7574766e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 4\tConfidence: [[5.5326305e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 3\tConfidence: [[3.417726e-05]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 2\tConfidence: [[0.00013826]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 1\tConfidence: [[0.00011278]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "Step: 0\tConfidence: [[0.00022715]]\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "AUC: 0.32369069571554226\n",
      "Average AUC: 0.32369069571554226\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images_deletion\"\n",
    "\n",
    "count = 0\n",
    "sum_auc = 0\n",
    "for filename in random_image_names:\n",
    "    confidences = []\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    for i in range(100,-1,-1):\n",
    "\n",
    "        if(i == 0):\n",
    "            gray_scale = np.zeros_like(gray_scale_original)\n",
    "        elif(i == 0):\n",
    "            gray_scale = np.ones_like(gray_scale_original)\n",
    "        else:\n",
    "            gray_scale = gray_scale_original.copy()\n",
    "            threshold = np.percentile(gray_scale, i)\n",
    "            gray_scale = gray_scale <= threshold\n",
    "\n",
    "        # Load and preprocess the image for CAM overlay\n",
    "        rgb_img = Image.open(img_path).convert('RGB')\n",
    "        rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "        img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "        # Calculate predicted class and confidence score change\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        # score = scores[0]\n",
    "        # visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        # visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, f\"{os.path.splitext(filename)[0]}_{i}{os.path.splitext(filename)[1]}\"))\n",
    "        confidences.append(scores_after)\n",
    "\n",
    "        # Print individual results\n",
    "        print(f\"Image: {filename}\")\n",
    "        print(f\"Step: {i}\\tConfidence: {scores_after}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "    confidences = np.array(confidences).flatten()\n",
    "    confidences = (confidences - confidences.min()) / (confidences.max() - confidences.min())\n",
    "    fraction_revealed = np.linspace(0, 1, len(confidences))\n",
    "    auc = np.trapz(confidences, fraction_revealed)\n",
    "    count += 1\n",
    "    sum_auc += auc\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "print(f\"Average AUC: {auc/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad CAM ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([22])\n",
      "Target class: bald_eagle\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "AUC: 0.8760039091715589\n",
      "Average AUC: 0.8760039091715589\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM_PLUS_PLUS\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images_insertion\"\n",
    "\n",
    "count = 0\n",
    "sum_auc = 0\n",
    "for filename in random_image_names:\n",
    "    confidences = []\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    for i in range(100,-1,-1):\n",
    "        if(i == 100):\n",
    "            gray_scale = np.zeros_like(gray_scale_original)\n",
    "        elif(i == 0):\n",
    "            gray_scale = np.ones_like(gray_scale_original)\n",
    "        else:\n",
    "            gray_scale = gray_scale_original.copy()\n",
    "            threshold = np.percentile(gray_scale, i)\n",
    "            gray_scale = gray_scale >= threshold\n",
    "\n",
    "        # Load and preprocess the image for CAM overlay\n",
    "        rgb_img = Image.open(img_path).convert('RGB')\n",
    "        rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "        img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "        # Calculate predicted class and confidence score change\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        # score = scores[0]\n",
    "        # visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        # visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, f\"{os.path.splitext(filename)[0]}_{i}{os.path.splitext(filename)[1]}\"))\n",
    "        confidences.append(scores_after)\n",
    "        \n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        # print(f\"Step: {i}\\tConfidence: {scores_after}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "    confidences = np.array(confidences).flatten()\n",
    "    confidences = (confidences - confidences.min()) / (confidences.max() - confidences.min())\n",
    "    fraction_revealed = np.linspace(0, 1, len(confidences))\n",
    "    auc = np.trapz(confidences, fraction_revealed)\n",
    "    count += 1\n",
    "    sum_auc += auc\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "print(f\"Average AUC: {sum_auc/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([22])\n",
      "Target class: bald_eagle\n",
      "Image: ILSVRC2012_val_00039605.JPEG\n",
      "AUC: 0.3327720255306122\n",
      "Average AUC: 0.3327720255306122\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM_PLUS_PLUS\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images_deletion\"\n",
    "\n",
    "count = 0\n",
    "sum_auc = 0\n",
    "for filename in random_image_names:\n",
    "    confidences = []\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    for i in range(100,-1,-1):\n",
    "\n",
    "        if(i == 0):\n",
    "            gray_scale = np.zeros_like(gray_scale_original)\n",
    "        elif(i == 0):\n",
    "            gray_scale = np.ones_like(gray_scale_original)\n",
    "        else:\n",
    "            gray_scale = gray_scale_original.copy()\n",
    "            threshold = np.percentile(gray_scale, i)\n",
    "            gray_scale = gray_scale <= threshold\n",
    "\n",
    "        # Load and preprocess the image for CAM overlay\n",
    "        rgb_img = Image.open(img_path).convert('RGB')\n",
    "        rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "        img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "        # Calculate predicted class and confidence score change\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        # score = scores[0]\n",
    "        # visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        # visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, f\"{os.path.splitext(filename)[0]}_{i}{os.path.splitext(filename)[1]}\"))\n",
    "        confidences.append(scores_after)\n",
    "\n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        # print(f\"Step: {i}\\tConfidence: {scores_after}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "    confidences = np.array(confidences).flatten()\n",
    "    confidences = (confidences - confidences.min()) / (confidences.max() - confidences.min())\n",
    "    fraction_revealed = np.linspace(0, 1, len(confidences))\n",
    "    auc = np.trapz(confidences, fraction_revealed)\n",
    "    count += 1\n",
    "    sum_auc += auc\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "print(f\"Average AUC: {auc/count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "score-cam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
