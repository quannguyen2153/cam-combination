{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from typing import Union, List\n",
    "from enum import Enum\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_grad_cam as cam\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, ClassifierOutputSoftmaxTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "class CAMType(Enum):\n",
    "    GRAD_CAM = cam.GradCAM\n",
    "    HI_RES_CAM = cam.HiResCAM\n",
    "    GRAD_CAM_ELEMENT_WISE = cam.GradCAMElementWise\n",
    "    ABLATION_CAM = cam.AblationCAM\n",
    "    X_GRAD_CAM = cam.XGradCAM\n",
    "    GRAD_CAM_PLUS_PLUS = cam.GradCAMPlusPlus\n",
    "    SCORE_CAM = cam.ScoreCAM\n",
    "    LAYER_CAM = cam.LayerCAM\n",
    "    EIGEN_CAM = cam.EigenCAM\n",
    "    EIGEN_GRAD_CAM = cam.EigenGradCAM\n",
    "    KPCA_CAM = cam.KPCA_CAM\n",
    "    RANDOM_CAM = cam.RandomCAM\n",
    "    FULL_GRAD = cam.FullGrad\n",
    "\n",
    "from typing import List, Callable\n",
    "\n",
    "def get_cam(cam_type: CAMType, model: torch.nn.Module, target_layers: torch.nn.Module):\n",
    "    cam_class = cam_type.value\n",
    "    return cam_class(model=model, target_layers=target_layers)\n",
    "\n",
    "def visualize(grayscale: torch.Tensor, rgb_img: np.ndarray):\n",
    "    visualization = show_cam_on_image(rgb_img, grayscale, use_rgb=True)\n",
    "    plt.imshow(visualization)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display_images_in_rows(rgb_img: np.ndarray, grayscale_list: List[torch.Tensor], labels: List[str]):\n",
    "    \"\"\"\n",
    "    Displays a list of images in rows, with each row containing up to max_images_per_row images.\n",
    "    \n",
    "    Args:\n",
    "        images (list of np.ndarray): List of images to display.\n",
    "        labels (list of str): List of labels for each image.\n",
    "        max_images_per_row (int): Maximum number of images per row (default is 4).\n",
    "    \"\"\"\n",
    "\n",
    "    images = [rgb_img]\n",
    "\n",
    "    for grayscale in grayscale_list:\n",
    "        images.append(show_cam_on_image(rgb_img, grayscale, use_rgb=True))\n",
    "\n",
    "    labels.insert(0, \"Image\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "    # Loop through the images and labels to display them\n",
    "    for ax, img, label in zip(axes, images, labels):\n",
    "        ax.imshow(img)  # Show the image\n",
    "        ax.set_title(label)  # Set the title\n",
    "        ax.axis('off')  # Hide axes\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def execute_cam(\n",
    "        img_path: str,\n",
    "        model: torch.nn.Module,\n",
    "        target_layers: List[torch.nn.Module],\n",
    "        cam_type: CAMType,\n",
    "        targets: Union[List[ClassifierOutputTarget], None] = None,\n",
    "        visualization: bool = False,\n",
    "        output_path: Union[str, None] = None\n",
    "    ):\n",
    "        model_name = \"unknown\"\n",
    "        target_layer_name = \"unknown\"\n",
    "        try:\n",
    "            model_name = model._get_name()\n",
    "            target_layer_name = target_layers[0]._get_name()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(f\"Executing CAM on {model_name} with target layer {target_layer_name}:\")\n",
    "\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = np.float32(img) / 255\n",
    "        input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        # Forward to get target class if not specified\n",
    "        if not targets:\n",
    "            with torch.no_grad():\n",
    "                predicted_class = model(input_tensor).max(1)[-1]\n",
    "                targets = [ClassifierOutputTarget(predicted_class)]\n",
    "                print(predicted_class)\n",
    "                print(f\"Target class: {labels[str(predicted_class.item())][1]}\")\n",
    "\n",
    "        cam = get_cam(cam_type=cam_type, model=model, target_layers=target_layers)\n",
    "\n",
    "        # Generate the CAM\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "        # Overlay the CAM on the image\n",
    "        if visualization:\n",
    "            # Convert the PIL image to a NumPy array for visualization\n",
    "            rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0  # Normalize the pixel values\n",
    "            \n",
    "            visualize(grayscale=grayscale_cam[0, :], rgb_img=rgb_img)\n",
    "\n",
    "            # Optionally, you can save the resulting CAM visualization\n",
    "            if output_path:\n",
    "                Image.fromarray((visualization * 255).astype(np.uint8)).save(output_path)\n",
    "                print(f\"Saved output image to {output_path}\")\n",
    "\n",
    "        return grayscale_cam, input_tensor\n",
    "\n",
    "labels = json.load(open(\"../pytorch_grad_cam/utils/imagenet_class_index.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbationConfidenceMetric:\n",
    "    def __init__(self, perturbation):\n",
    "        self.perturbation = perturbation\n",
    "\n",
    "    def __call__(self, input_tensor: torch.Tensor,\n",
    "                 cams: np.ndarray,\n",
    "                 targets: List[Callable],\n",
    "                 model: torch.nn.Module,\n",
    "                 return_visualization=False,\n",
    "                 return_diff=True):\n",
    "\n",
    "        if return_diff:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)\n",
    "                scores = [target(output).cpu().numpy()\n",
    "                          for target, output in zip(targets, outputs)]\n",
    "                scores = np.float32(scores)\n",
    "\n",
    "        batch_size = input_tensor.size(0)\n",
    "        perturbated_tensors = []\n",
    "        for i in range(batch_size):\n",
    "            cam = cams[i]\n",
    "            tensor = self.perturbation(input_tensor[i, ...].cpu(),\n",
    "                                       torch.from_numpy(cam))\n",
    "            tensor = tensor.to(input_tensor.device)\n",
    "            perturbated_tensors.append(tensor.unsqueeze(0))\n",
    "        perturbated_tensors = torch.cat(perturbated_tensors)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_after_imputation = model(perturbated_tensors)\n",
    "        scores_after_imputation = [\n",
    "            target(output).cpu().numpy() for target, output in zip(\n",
    "                targets, outputs_after_imputation)]\n",
    "        scores_after_imputation = np.float32(scores_after_imputation)\n",
    "\n",
    "        if return_diff:\n",
    "            result = scores_after_imputation - scores\n",
    "        else:\n",
    "            result = scores_after_imputation\n",
    "\n",
    "        if return_visualization:\n",
    "            return result, scores, scores_after_imputation, perturbated_tensors\n",
    "        else:\n",
    "            return result, scores, scores_after_imputation\n",
    "\n",
    "def multiply_tensor_with_cam(input_tensor: torch.Tensor,\n",
    "                             cam: torch.Tensor):\n",
    "    \"\"\" Multiply an input tensor (after normalization)\n",
    "        with a pixel attribution map\n",
    "    \"\"\"\n",
    "    return input_tensor * cam\n",
    "        \n",
    "class CamMultImageConfidenceChange(PerturbationConfidenceMetric):\n",
    "    def __init__(self):\n",
    "        super(CamMultImageConfidenceChange,\n",
    "              self).__init__(multiply_tensor_with_cam)\n",
    "        \n",
    "class DropInConfidence(CamMultImageConfidenceChange):\n",
    "    def __init__(self):\n",
    "        super(DropInConfidence, self).__init__()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        scores, scores_before, scores_after = super(DropInConfidence, self).__call__(*args, **kwargs)\n",
    "        scores = -scores\n",
    "        return np.maximum(scores, 0) / scores_before * 100\n",
    "\n",
    "\n",
    "class IncreaseInConfidence(CamMultImageConfidenceChange):\n",
    "    def __init__(self):\n",
    "        super(IncreaseInConfidence, self).__init__()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        scores, bef_score, scores_after = super(IncreaseInConfidence, self).__call__(*args, **kwargs)\n",
    "        return np.float32(scores > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model & method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights=models.VGG16_Weights.DEFAULT).eval()\n",
    "target_layers = [model.features[28]]\n",
    "\n",
    "cam_type = CAMType.GRAD_CAM_PLUS_PLUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average drop & Average increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "input_folder = r\"C:\\Users\\HaPham\\Documents\\ThesisXAI\\Code\\CAM-combination\\ILSVRC2012\\ILSVRC2012_img_val\"\n",
    "imputated_folder = \"../imputated_images\"\n",
    "\n",
    "if os.path.exists(imputated_folder):\n",
    "    shutil.rmtree(imputated_folder)\n",
    "os.makedirs(imputated_folder)\n",
    "\n",
    "# Generate a list of image names within the specified range\n",
    "start_idx = 1\n",
    "end_idx = 50000\n",
    "num_images_to_sample = 1  # Adjust this to how many random images you want\n",
    "\n",
    "all_image_names = [\n",
    "    f\"ILSVRC2012_val_{i:08d}.JPEG\" for i in range(start_idx, end_idx + 1)\n",
    "]\n",
    "random_image_names = random.sample(all_image_names, num_images_to_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([437])\n",
      "Target class: beacon\n",
      "Image: ILSVRC2012_val_00035796.JPEG\n",
      "Confidence 100% img: [[0.99212414]]\n",
      "Confidence 0% img: [[0.00054411]]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images\"\n",
    "for filename in random_image_names:\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    gray_scale = np.zeros_like(gray_scale_original)\n",
    "\n",
    "    # Load and preprocess the image for CAM overlay\n",
    "    rgb_img = Image.open(img_path).convert('RGB')\n",
    "    rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "    img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "    # Calculate predicted class and confidence score change\n",
    "    predicted_class = model(input_tensor).max(1)[-1]\n",
    "    # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "    targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "    scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "        input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "    )\n",
    "\n",
    "    # Process the visualization for display and scoring\n",
    "    score = scores[0]\n",
    "    visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "    visualization = deprocess_image(visualization)\n",
    "\n",
    "    # Save the visualization\n",
    "    # Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "    # Print individual results\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"Confidence 100% img: {scores_before}\")\n",
    "    print(f\"Confidence 0% img: {scores_after}\")\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([437])\n",
      "Target class: beacon\n",
      "Step: 100\tConfidence: [[0.00054411]]\n",
      "Step: 99\tConfidence: [[5.582654e-05]]\n",
      "Step: 98\tConfidence: [[1.9907677e-05]]\n",
      "Step: 97\tConfidence: [[3.3090324e-05]]\n",
      "Step: 96\tConfidence: [[3.387173e-05]]\n",
      "Step: 95\tConfidence: [[2.9433377e-05]]\n",
      "Step: 94\tConfidence: [[4.498452e-05]]\n",
      "Step: 93\tConfidence: [[1.8493047e-05]]\n",
      "Step: 92\tConfidence: [[1.4018758e-05]]\n",
      "Step: 91\tConfidence: [[7.7983095e-06]]\n",
      "Step: 90\tConfidence: [[9.7962475e-06]]\n",
      "Step: 89\tConfidence: [[2.4968918e-05]]\n",
      "Step: 88\tConfidence: [[1.3648781e-05]]\n",
      "Step: 87\tConfidence: [[1.9765677e-05]]\n",
      "Step: 86\tConfidence: [[2.8032608e-05]]\n",
      "Step: 85\tConfidence: [[7.168249e-05]]\n",
      "Step: 84\tConfidence: [[0.00020679]]\n",
      "Step: 83\tConfidence: [[0.00015185]]\n",
      "Step: 82\tConfidence: [[0.00022782]]\n",
      "Step: 81\tConfidence: [[0.0003084]]\n",
      "Step: 80\tConfidence: [[0.00033685]]\n",
      "Step: 79\tConfidence: [[0.00042514]]\n",
      "Step: 78\tConfidence: [[0.00064717]]\n",
      "Step: 77\tConfidence: [[0.00087827]]\n",
      "Step: 76\tConfidence: [[0.00085329]]\n",
      "Step: 75\tConfidence: [[0.00779177]]\n",
      "Step: 74\tConfidence: [[0.00846279]]\n",
      "Step: 73\tConfidence: [[0.01643137]]\n",
      "Step: 72\tConfidence: [[0.03194898]]\n",
      "Step: 71\tConfidence: [[0.03522858]]\n",
      "Step: 70\tConfidence: [[0.01746056]]\n",
      "Step: 69\tConfidence: [[0.03041634]]\n",
      "Step: 68\tConfidence: [[0.03334117]]\n",
      "Step: 67\tConfidence: [[0.03440979]]\n",
      "Step: 66\tConfidence: [[0.04005647]]\n",
      "Step: 65\tConfidence: [[0.05364861]]\n",
      "Step: 64\tConfidence: [[0.07137009]]\n",
      "Step: 63\tConfidence: [[0.07551953]]\n",
      "Step: 62\tConfidence: [[0.09323997]]\n",
      "Step: 61\tConfidence: [[0.07251029]]\n",
      "Step: 60\tConfidence: [[0.06349222]]\n",
      "Step: 59\tConfidence: [[0.03793179]]\n",
      "Step: 58\tConfidence: [[0.0301073]]\n",
      "Step: 57\tConfidence: [[0.01582105]]\n",
      "Step: 56\tConfidence: [[0.02352712]]\n",
      "Step: 55\tConfidence: [[0.02730633]]\n",
      "Step: 54\tConfidence: [[0.03185623]]\n",
      "Step: 53\tConfidence: [[0.04805907]]\n",
      "Step: 52\tConfidence: [[0.02261183]]\n",
      "Step: 51\tConfidence: [[0.0436836]]\n",
      "Step: 50\tConfidence: [[0.05567513]]\n",
      "Step: 49\tConfidence: [[0.07933987]]\n",
      "Step: 48\tConfidence: [[0.07706671]]\n",
      "Step: 47\tConfidence: [[0.06648175]]\n",
      "Step: 46\tConfidence: [[0.10740364]]\n",
      "Step: 45\tConfidence: [[0.18292862]]\n",
      "Step: 44\tConfidence: [[0.13790084]]\n",
      "Step: 43\tConfidence: [[0.19173521]]\n",
      "Step: 42\tConfidence: [[0.281003]]\n",
      "Step: 41\tConfidence: [[0.24621627]]\n",
      "Step: 40\tConfidence: [[0.20020199]]\n",
      "Step: 39\tConfidence: [[0.23520866]]\n",
      "Step: 38\tConfidence: [[0.2503475]]\n",
      "Step: 37\tConfidence: [[0.11928377]]\n",
      "Step: 36\tConfidence: [[0.27977332]]\n",
      "Step: 35\tConfidence: [[0.1317097]]\n",
      "Step: 34\tConfidence: [[0.12436648]]\n",
      "Step: 33\tConfidence: [[0.25464758]]\n",
      "Step: 32\tConfidence: [[0.171841]]\n",
      "Step: 31\tConfidence: [[0.21292822]]\n",
      "Step: 30\tConfidence: [[0.3049042]]\n",
      "Step: 29\tConfidence: [[0.3773229]]\n",
      "Step: 28\tConfidence: [[0.5199674]]\n",
      "Step: 27\tConfidence: [[0.8004707]]\n",
      "Step: 26\tConfidence: [[0.79990435]]\n",
      "Step: 25\tConfidence: [[0.7600167]]\n",
      "Step: 24\tConfidence: [[0.79863995]]\n",
      "Step: 23\tConfidence: [[0.81611884]]\n",
      "Step: 22\tConfidence: [[0.79355174]]\n",
      "Step: 21\tConfidence: [[0.85394514]]\n",
      "Step: 20\tConfidence: [[0.87191236]]\n",
      "Step: 19\tConfidence: [[0.87458086]]\n",
      "Step: 18\tConfidence: [[0.8783626]]\n",
      "Step: 17\tConfidence: [[0.8879957]]\n",
      "Step: 16\tConfidence: [[0.8590406]]\n",
      "Step: 15\tConfidence: [[0.866765]]\n",
      "Step: 14\tConfidence: [[0.845356]]\n",
      "Step: 13\tConfidence: [[0.74156463]]\n",
      "Step: 12\tConfidence: [[0.77767605]]\n",
      "Step: 11\tConfidence: [[0.7971718]]\n",
      "Step: 10\tConfidence: [[0.6051372]]\n",
      "Step: 9\tConfidence: [[0.6221047]]\n",
      "Step: 8\tConfidence: [[0.74174476]]\n",
      "Step: 7\tConfidence: [[0.5129462]]\n",
      "Step: 6\tConfidence: [[0.8644735]]\n",
      "Step: 5\tConfidence: [[0.95837945]]\n",
      "Step: 4\tConfidence: [[0.98641425]]\n",
      "Step: 3\tConfidence: [[0.98859847]]\n",
      "Step: 2\tConfidence: [[0.99560577]]\n",
      "Step: 1\tConfidence: [[0.9973181]]\n",
      "Step: 0\tConfidence: [[0.99212414]]\n",
      "Image: ILSVRC2012_val_00035796.JPEG\n",
      "AUC: 0.28371077170682835\n",
      "Average AUC: 0.28371077170682835\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images_insertion\"\n",
    "\n",
    "count = 0\n",
    "sum_auc = 0\n",
    "for filename in random_image_names:\n",
    "    confidences = []\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    for i in range(100,-1,-1):\n",
    "        if(i == 100):\n",
    "            gray_scale = np.zeros_like(gray_scale_original)\n",
    "        else:\n",
    "            gray_scale = gray_scale_original.copy()\n",
    "            threshold = np.percentile(gray_scale, i)\n",
    "            gray_scale = gray_scale >= threshold\n",
    "\n",
    "        # Load and preprocess the image for CAM overlay\n",
    "        rgb_img = Image.open(img_path).convert('RGB')\n",
    "        rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "        img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "        # Calculate predicted class and confidence score change\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        # score = scores[0]\n",
    "        # visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        # visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, f\"{os.path.splitext(filename)[0]}_{i}{os.path.splitext(filename)[1]}\"))\n",
    "        confidences.append(scores_after)\n",
    "        \n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        print(f\"Step: {i}\\tConfidence: {scores_after}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "    confidences = np.array(confidences).flatten()\n",
    "    fraction_revealed = np.linspace(0, 1, len(confidences))\n",
    "    auc = np.trapz(confidences, fraction_revealed)\n",
    "    count += 1\n",
    "    sum_auc += auc\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "print(f\"Average AUC: {sum_auc/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([437])\n",
      "Target class: beacon\n",
      "Step: 100\tConfidence: [[0.99212414]]\n",
      "Step: 99\tConfidence: [[0.9854188]]\n",
      "Step: 98\tConfidence: [[0.8821548]]\n",
      "Step: 97\tConfidence: [[0.7897765]]\n",
      "Step: 96\tConfidence: [[0.6239024]]\n",
      "Step: 95\tConfidence: [[0.45257887]]\n",
      "Step: 94\tConfidence: [[0.33952564]]\n",
      "Step: 93\tConfidence: [[0.2147102]]\n",
      "Step: 92\tConfidence: [[0.20704406]]\n",
      "Step: 91\tConfidence: [[0.12032034]]\n",
      "Step: 90\tConfidence: [[0.09610799]]\n",
      "Step: 89\tConfidence: [[0.13753752]]\n",
      "Step: 88\tConfidence: [[0.10467155]]\n",
      "Step: 87\tConfidence: [[0.1489806]]\n",
      "Step: 86\tConfidence: [[0.07834413]]\n",
      "Step: 85\tConfidence: [[0.05597859]]\n",
      "Step: 84\tConfidence: [[0.03986201]]\n",
      "Step: 83\tConfidence: [[0.01712495]]\n",
      "Step: 82\tConfidence: [[0.01766221]]\n",
      "Step: 81\tConfidence: [[0.01451036]]\n",
      "Step: 80\tConfidence: [[0.00874836]]\n",
      "Step: 79\tConfidence: [[0.01192575]]\n",
      "Step: 78\tConfidence: [[0.00652318]]\n",
      "Step: 77\tConfidence: [[0.01598031]]\n",
      "Step: 76\tConfidence: [[0.01021038]]\n",
      "Step: 75\tConfidence: [[0.02158858]]\n",
      "Step: 74\tConfidence: [[0.02402207]]\n",
      "Step: 73\tConfidence: [[0.05264565]]\n",
      "Step: 72\tConfidence: [[0.04932236]]\n",
      "Step: 71\tConfidence: [[0.07230046]]\n",
      "Step: 70\tConfidence: [[0.05241689]]\n",
      "Step: 69\tConfidence: [[0.0643326]]\n",
      "Step: 68\tConfidence: [[0.10745434]]\n",
      "Step: 67\tConfidence: [[0.13700965]]\n",
      "Step: 66\tConfidence: [[0.14776489]]\n",
      "Step: 65\tConfidence: [[0.0874269]]\n",
      "Step: 64\tConfidence: [[0.1345471]]\n",
      "Step: 63\tConfidence: [[0.11721575]]\n",
      "Step: 62\tConfidence: [[0.10034543]]\n",
      "Step: 61\tConfidence: [[0.11940237]]\n",
      "Step: 60\tConfidence: [[0.1527483]]\n",
      "Step: 59\tConfidence: [[0.1031873]]\n",
      "Step: 58\tConfidence: [[0.15333283]]\n",
      "Step: 57\tConfidence: [[0.11173189]]\n",
      "Step: 56\tConfidence: [[0.12049046]]\n",
      "Step: 55\tConfidence: [[0.23049669]]\n",
      "Step: 54\tConfidence: [[0.11191801]]\n",
      "Step: 53\tConfidence: [[0.05512669]]\n",
      "Step: 52\tConfidence: [[0.04950421]]\n",
      "Step: 51\tConfidence: [[0.02382894]]\n",
      "Step: 50\tConfidence: [[0.01658322]]\n",
      "Step: 49\tConfidence: [[0.01614829]]\n",
      "Step: 48\tConfidence: [[0.01373905]]\n",
      "Step: 47\tConfidence: [[0.00941889]]\n",
      "Step: 46\tConfidence: [[0.00409729]]\n",
      "Step: 45\tConfidence: [[0.00398172]]\n",
      "Step: 44\tConfidence: [[0.0023202]]\n",
      "Step: 43\tConfidence: [[0.00453332]]\n",
      "Step: 42\tConfidence: [[0.00538852]]\n",
      "Step: 41\tConfidence: [[0.0025655]]\n",
      "Step: 40\tConfidence: [[0.00439294]]\n",
      "Step: 39\tConfidence: [[0.00381286]]\n",
      "Step: 38\tConfidence: [[0.0090703]]\n",
      "Step: 37\tConfidence: [[0.00292661]]\n",
      "Step: 36\tConfidence: [[0.00335103]]\n",
      "Step: 35\tConfidence: [[0.00199119]]\n",
      "Step: 34\tConfidence: [[0.00095604]]\n",
      "Step: 33\tConfidence: [[0.00485787]]\n",
      "Step: 32\tConfidence: [[0.01037485]]\n",
      "Step: 31\tConfidence: [[0.0129997]]\n",
      "Step: 30\tConfidence: [[0.02968317]]\n",
      "Step: 29\tConfidence: [[0.02770625]]\n",
      "Step: 28\tConfidence: [[0.01728426]]\n",
      "Step: 27\tConfidence: [[0.03548042]]\n",
      "Step: 26\tConfidence: [[0.03274078]]\n",
      "Step: 25\tConfidence: [[0.02283806]]\n",
      "Step: 24\tConfidence: [[0.03167534]]\n",
      "Step: 23\tConfidence: [[0.02717987]]\n",
      "Step: 22\tConfidence: [[0.02029812]]\n",
      "Step: 21\tConfidence: [[0.03402498]]\n",
      "Step: 20\tConfidence: [[0.0254817]]\n",
      "Step: 19\tConfidence: [[0.01366807]]\n",
      "Step: 18\tConfidence: [[0.00984701]]\n",
      "Step: 17\tConfidence: [[0.01007387]]\n",
      "Step: 16\tConfidence: [[0.00814512]]\n",
      "Step: 15\tConfidence: [[0.01133096]]\n",
      "Step: 14\tConfidence: [[0.00743843]]\n",
      "Step: 13\tConfidence: [[0.0058352]]\n",
      "Step: 12\tConfidence: [[0.00485021]]\n",
      "Step: 11\tConfidence: [[0.00311338]]\n",
      "Step: 10\tConfidence: [[0.00510599]]\n",
      "Step: 9\tConfidence: [[0.01410464]]\n",
      "Step: 8\tConfidence: [[0.01327946]]\n",
      "Step: 7\tConfidence: [[0.01077259]]\n",
      "Step: 6\tConfidence: [[0.02305106]]\n",
      "Step: 5\tConfidence: [[0.01419884]]\n",
      "Step: 4\tConfidence: [[0.00992836]]\n",
      "Step: 3\tConfidence: [[0.01316966]]\n",
      "Step: 2\tConfidence: [[0.52101403]]\n",
      "Step: 1\tConfidence: [[0.2874374]]\n",
      "Step: 0\tConfidence: [[0.00054411]]\n",
      "Image: ILSVRC2012_val_00035796.JPEG\n",
      "AUC: 0.09700360525515862\n",
      "Average AUC: 0.04850180262757931\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images_deletion\"\n",
    "\n",
    "for filename in random_image_names:\n",
    "    confidences = []\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    for i in range(100,-1,-1):\n",
    "\n",
    "        if(i == 0):\n",
    "            gray_scale = np.zeros_like(gray_scale_original)\n",
    "        else:\n",
    "            gray_scale = gray_scale_original.copy()\n",
    "            threshold = np.percentile(gray_scale, i)\n",
    "            gray_scale = gray_scale <= threshold\n",
    "\n",
    "        # Load and preprocess the image for CAM overlay\n",
    "        rgb_img = Image.open(img_path).convert('RGB')\n",
    "        rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "        img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "        # Calculate predicted class and confidence score change\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        # score = scores[0]\n",
    "        # visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        # visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, f\"{os.path.splitext(filename)[0]}_{i}{os.path.splitext(filename)[1]}\"))\n",
    "        confidences.append(scores_after)\n",
    "\n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        print(f\"Step: {i}\\tConfidence: {scores_after}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "    confidences = np.array(confidences).flatten()\n",
    "    fraction_revealed = np.linspace(0, 1, len(confidences))\n",
    "    auc = np.trapz(confidences, fraction_revealed)\n",
    "    count += 1\n",
    "    sum_auc += auc\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "print(f\"Average AUC: {auc/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([437])\n",
      "Target class: beacon\n",
      "Step: 100\tConfidence: [[0.00054411]]\n",
      "Step: 99\tConfidence: [[8.5310625e-05]]\n",
      "Step: 98\tConfidence: [[2.2042052e-05]]\n",
      "Step: 97\tConfidence: [[5.429373e-05]]\n",
      "Step: 96\tConfidence: [[0.0002955]]\n",
      "Step: 95\tConfidence: [[0.00030456]]\n",
      "Step: 94\tConfidence: [[0.000789]]\n",
      "Step: 93\tConfidence: [[0.00021425]]\n",
      "Step: 92\tConfidence: [[0.0002068]]\n",
      "Step: 91\tConfidence: [[0.00015625]]\n",
      "Step: 90\tConfidence: [[0.00014574]]\n",
      "Step: 89\tConfidence: [[0.00028496]]\n",
      "Step: 88\tConfidence: [[0.00028745]]\n",
      "Step: 87\tConfidence: [[0.00034626]]\n",
      "Step: 86\tConfidence: [[0.00061004]]\n",
      "Step: 85\tConfidence: [[0.00236902]]\n",
      "Step: 84\tConfidence: [[0.00121188]]\n",
      "Step: 83\tConfidence: [[0.00383222]]\n",
      "Step: 82\tConfidence: [[0.00436492]]\n",
      "Step: 81\tConfidence: [[0.00271749]]\n",
      "Step: 80\tConfidence: [[0.00254508]]\n",
      "Step: 79\tConfidence: [[0.00370423]]\n",
      "Step: 78\tConfidence: [[0.00219308]]\n",
      "Step: 77\tConfidence: [[0.0021805]]\n",
      "Step: 76\tConfidence: [[0.00537535]]\n",
      "Step: 75\tConfidence: [[0.00806968]]\n",
      "Step: 74\tConfidence: [[0.00666592]]\n",
      "Step: 73\tConfidence: [[0.00835715]]\n",
      "Step: 72\tConfidence: [[0.0116797]]\n",
      "Step: 71\tConfidence: [[0.0269552]]\n",
      "Step: 70\tConfidence: [[0.01982813]]\n",
      "Step: 69\tConfidence: [[0.03424455]]\n",
      "Step: 68\tConfidence: [[0.03860629]]\n",
      "Step: 67\tConfidence: [[0.03950813]]\n",
      "Step: 66\tConfidence: [[0.03210821]]\n",
      "Step: 65\tConfidence: [[0.03403106]]\n",
      "Step: 64\tConfidence: [[0.02488209]]\n",
      "Step: 63\tConfidence: [[0.02777755]]\n",
      "Step: 62\tConfidence: [[0.02968557]]\n",
      "Step: 61\tConfidence: [[0.05023342]]\n",
      "Step: 60\tConfidence: [[0.05127338]]\n",
      "Step: 59\tConfidence: [[0.03404747]]\n",
      "Step: 58\tConfidence: [[0.05123849]]\n",
      "Step: 57\tConfidence: [[0.04770472]]\n",
      "Step: 56\tConfidence: [[0.0461808]]\n",
      "Step: 55\tConfidence: [[0.02683822]]\n",
      "Step: 54\tConfidence: [[0.03342969]]\n",
      "Step: 53\tConfidence: [[0.0173537]]\n",
      "Step: 52\tConfidence: [[0.03070822]]\n",
      "Step: 51\tConfidence: [[0.0281999]]\n",
      "Step: 50\tConfidence: [[0.022622]]\n",
      "Step: 49\tConfidence: [[0.01718156]]\n",
      "Step: 48\tConfidence: [[0.02222321]]\n",
      "Step: 47\tConfidence: [[0.01026077]]\n",
      "Step: 46\tConfidence: [[0.02625645]]\n",
      "Step: 45\tConfidence: [[0.05111096]]\n",
      "Step: 44\tConfidence: [[0.0509086]]\n",
      "Step: 43\tConfidence: [[0.11000119]]\n",
      "Step: 42\tConfidence: [[0.09030571]]\n",
      "Step: 41\tConfidence: [[0.20068078]]\n",
      "Step: 40\tConfidence: [[0.23699825]]\n",
      "Step: 39\tConfidence: [[0.3218116]]\n",
      "Step: 38\tConfidence: [[0.25983125]]\n",
      "Step: 37\tConfidence: [[0.31905827]]\n",
      "Step: 36\tConfidence: [[0.45435563]]\n",
      "Step: 35\tConfidence: [[0.48367825]]\n",
      "Step: 34\tConfidence: [[0.5596097]]\n",
      "Step: 33\tConfidence: [[0.54175305]]\n",
      "Step: 32\tConfidence: [[0.6195422]]\n",
      "Step: 31\tConfidence: [[0.5709066]]\n",
      "Step: 30\tConfidence: [[0.5493207]]\n",
      "Step: 29\tConfidence: [[0.542009]]\n",
      "Step: 28\tConfidence: [[0.5393057]]\n",
      "Step: 27\tConfidence: [[0.625022]]\n",
      "Step: 26\tConfidence: [[0.61104965]]\n",
      "Step: 25\tConfidence: [[0.5032992]]\n",
      "Step: 24\tConfidence: [[0.49897185]]\n",
      "Step: 23\tConfidence: [[0.43715963]]\n",
      "Step: 22\tConfidence: [[0.7314366]]\n",
      "Step: 21\tConfidence: [[0.73903894]]\n",
      "Step: 20\tConfidence: [[0.8037229]]\n",
      "Step: 19\tConfidence: [[0.8064233]]\n",
      "Step: 18\tConfidence: [[0.7478508]]\n",
      "Step: 17\tConfidence: [[0.90340126]]\n",
      "Step: 16\tConfidence: [[0.9160154]]\n",
      "Step: 15\tConfidence: [[0.91048056]]\n",
      "Step: 14\tConfidence: [[0.92094964]]\n",
      "Step: 13\tConfidence: [[0.88713855]]\n",
      "Step: 12\tConfidence: [[0.90700245]]\n",
      "Step: 11\tConfidence: [[0.9303112]]\n",
      "Step: 10\tConfidence: [[0.9160758]]\n",
      "Step: 9\tConfidence: [[0.90198815]]\n",
      "Step: 8\tConfidence: [[0.91188014]]\n",
      "Step: 7\tConfidence: [[0.9513221]]\n",
      "Step: 6\tConfidence: [[0.9165254]]\n",
      "Step: 5\tConfidence: [[0.85625905]]\n",
      "Step: 4\tConfidence: [[0.8131749]]\n",
      "Step: 3\tConfidence: [[0.8358207]]\n",
      "Step: 2\tConfidence: [[0.7713777]]\n",
      "Step: 1\tConfidence: [[0.8776376]]\n",
      "Step: 0\tConfidence: [[0.99212414]]\n",
      "Image: ILSVRC2012_val_00035796.JPEG\n",
      "AUC: 0.2952130430857505\n",
      "Average AUC: 0.2952130430857505\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM_PLUS_PLUS\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images_insertion\"\n",
    "\n",
    "count = 0\n",
    "sum_auc = 0\n",
    "for filename in random_image_names:\n",
    "    confidences = []\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    for i in range(100,-1,-1):\n",
    "        if(i == 100):\n",
    "            gray_scale = np.zeros_like(gray_scale_original)\n",
    "        else:\n",
    "            gray_scale = gray_scale_original.copy()\n",
    "            threshold = np.percentile(gray_scale, i)\n",
    "            gray_scale = gray_scale >= threshold\n",
    "\n",
    "        # Load and preprocess the image for CAM overlay\n",
    "        rgb_img = Image.open(img_path).convert('RGB')\n",
    "        rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "        img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "        # Calculate predicted class and confidence score change\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        # score = scores[0]\n",
    "        # visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        # visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, f\"{os.path.splitext(filename)[0]}_{i}{os.path.splitext(filename)[1]}\"))\n",
    "        confidences.append(scores_after)\n",
    "        \n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        print(f\"Step: {i}\\tConfidence: {scores_after}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "    confidences = np.array(confidences).flatten()\n",
    "    fraction_revealed = np.linspace(0, 1, len(confidences))\n",
    "    auc = np.trapz(confidences, fraction_revealed)\n",
    "    count += 1\n",
    "    sum_auc += auc\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "print(f\"Average AUC: {sum_auc/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing CAM on VGG with target layer Conv2d:\n",
      "tensor([437])\n",
      "Target class: beacon\n",
      "Step: 100\tConfidence: [[0.99212414]]\n",
      "Step: 99\tConfidence: [[0.98058426]]\n",
      "Step: 98\tConfidence: [[0.9645279]]\n",
      "Step: 97\tConfidence: [[0.9607358]]\n",
      "Step: 96\tConfidence: [[0.8899453]]\n",
      "Step: 95\tConfidence: [[0.7741756]]\n",
      "Step: 94\tConfidence: [[0.77725]]\n",
      "Step: 93\tConfidence: [[0.7021421]]\n",
      "Step: 92\tConfidence: [[0.5721567]]\n",
      "Step: 91\tConfidence: [[0.397932]]\n",
      "Step: 90\tConfidence: [[0.25656882]]\n",
      "Step: 89\tConfidence: [[0.21060997]]\n",
      "Step: 88\tConfidence: [[0.24392323]]\n",
      "Step: 87\tConfidence: [[0.20311794]]\n",
      "Step: 86\tConfidence: [[0.1448014]]\n",
      "Step: 85\tConfidence: [[0.18615104]]\n",
      "Step: 84\tConfidence: [[0.1146141]]\n",
      "Step: 83\tConfidence: [[0.06863662]]\n",
      "Step: 82\tConfidence: [[0.04599651]]\n",
      "Step: 81\tConfidence: [[0.06397569]]\n",
      "Step: 80\tConfidence: [[0.0433962]]\n",
      "Step: 79\tConfidence: [[0.03132259]]\n",
      "Step: 78\tConfidence: [[0.02279584]]\n",
      "Step: 77\tConfidence: [[0.01580017]]\n",
      "Step: 76\tConfidence: [[0.01183193]]\n",
      "Step: 75\tConfidence: [[0.00779056]]\n",
      "Step: 74\tConfidence: [[0.00658089]]\n",
      "Step: 73\tConfidence: [[0.00681804]]\n",
      "Step: 72\tConfidence: [[0.01328663]]\n",
      "Step: 71\tConfidence: [[0.01426266]]\n",
      "Step: 70\tConfidence: [[0.00819702]]\n",
      "Step: 69\tConfidence: [[0.00781226]]\n",
      "Step: 68\tConfidence: [[0.00557264]]\n",
      "Step: 67\tConfidence: [[0.00635975]]\n",
      "Step: 66\tConfidence: [[0.00817674]]\n",
      "Step: 65\tConfidence: [[0.00508055]]\n",
      "Step: 64\tConfidence: [[0.00717764]]\n",
      "Step: 63\tConfidence: [[0.00557189]]\n",
      "Step: 62\tConfidence: [[0.00436716]]\n",
      "Step: 61\tConfidence: [[0.0032503]]\n",
      "Step: 60\tConfidence: [[0.00216226]]\n",
      "Step: 59\tConfidence: [[0.00233206]]\n",
      "Step: 58\tConfidence: [[0.00191954]]\n",
      "Step: 57\tConfidence: [[0.00130476]]\n",
      "Step: 56\tConfidence: [[0.00137578]]\n",
      "Step: 55\tConfidence: [[0.00064296]]\n",
      "Step: 54\tConfidence: [[0.00092082]]\n",
      "Step: 53\tConfidence: [[0.00021036]]\n",
      "Step: 52\tConfidence: [[0.00032664]]\n",
      "Step: 51\tConfidence: [[0.00030571]]\n",
      "Step: 50\tConfidence: [[0.00025104]]\n",
      "Step: 49\tConfidence: [[0.00015336]]\n",
      "Step: 48\tConfidence: [[0.0005269]]\n",
      "Step: 47\tConfidence: [[0.0001097]]\n",
      "Step: 46\tConfidence: [[0.00023327]]\n",
      "Step: 45\tConfidence: [[0.00019419]]\n",
      "Step: 44\tConfidence: [[0.00027068]]\n",
      "Step: 43\tConfidence: [[0.0007792]]\n",
      "Step: 42\tConfidence: [[0.00047562]]\n",
      "Step: 41\tConfidence: [[0.00121457]]\n",
      "Step: 40\tConfidence: [[0.0012772]]\n",
      "Step: 39\tConfidence: [[0.00118432]]\n",
      "Step: 38\tConfidence: [[0.00084651]]\n",
      "Step: 37\tConfidence: [[0.00098562]]\n",
      "Step: 36\tConfidence: [[0.00080151]]\n",
      "Step: 35\tConfidence: [[0.00066346]]\n",
      "Step: 34\tConfidence: [[0.00032391]]\n",
      "Step: 33\tConfidence: [[0.00052076]]\n",
      "Step: 32\tConfidence: [[0.00050287]]\n",
      "Step: 31\tConfidence: [[0.00077596]]\n",
      "Step: 30\tConfidence: [[0.00110244]]\n",
      "Step: 29\tConfidence: [[0.00075952]]\n",
      "Step: 28\tConfidence: [[0.00217724]]\n",
      "Step: 27\tConfidence: [[0.00264782]]\n",
      "Step: 26\tConfidence: [[0.00400735]]\n",
      "Step: 25\tConfidence: [[0.0036583]]\n",
      "Step: 24\tConfidence: [[0.00387454]]\n",
      "Step: 23\tConfidence: [[0.0010759]]\n",
      "Step: 22\tConfidence: [[0.001824]]\n",
      "Step: 21\tConfidence: [[0.00540513]]\n",
      "Step: 20\tConfidence: [[0.0058208]]\n",
      "Step: 19\tConfidence: [[0.00822707]]\n",
      "Step: 18\tConfidence: [[0.00711696]]\n",
      "Step: 17\tConfidence: [[0.01680779]]\n",
      "Step: 16\tConfidence: [[0.01947288]]\n",
      "Step: 15\tConfidence: [[0.02926694]]\n",
      "Step: 14\tConfidence: [[0.0106497]]\n",
      "Step: 13\tConfidence: [[0.00898864]]\n",
      "Step: 12\tConfidence: [[0.00640968]]\n",
      "Step: 11\tConfidence: [[0.00262926]]\n",
      "Step: 10\tConfidence: [[0.00050978]]\n",
      "Step: 9\tConfidence: [[0.0001059]]\n",
      "Step: 8\tConfidence: [[8.995995e-05]]\n",
      "Step: 7\tConfidence: [[0.0001909]]\n",
      "Step: 6\tConfidence: [[4.132746e-05]]\n",
      "Step: 5\tConfidence: [[1.1963133e-05]]\n",
      "Step: 4\tConfidence: [[2.9118472e-05]]\n",
      "Step: 3\tConfidence: [[1.190751e-05]]\n",
      "Step: 2\tConfidence: [[2.039726e-05]]\n",
      "Step: 1\tConfidence: [[0.00010726]]\n",
      "Step: 0\tConfidence: [[0.00054411]]\n",
      "Image: ILSVRC2012_val_00035796.JPEG\n",
      "AUC: 0.09456262676956613\n",
      "Average AUC: 0.047281313384783065\n"
     ]
    }
   ],
   "source": [
    "cam_type = CAMType.GRAD_CAM_PLUS_PLUS\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "drop_in_conf_metric = DropInConfidence()\n",
    "increase_in_conf_metric = IncreaseInConfidence()\n",
    "imputated_folder = \"../imputated_images_deletion\"\n",
    "\n",
    "for filename in random_image_names:\n",
    "    confidences = []\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    gray_scale_original, input_tensor = execute_cam(img_path=img_path, model=model, target_layers=target_layers, cam_type=cam_type)\n",
    "\n",
    "    for i in range(100,-1,-1):\n",
    "\n",
    "        if(i == 0):\n",
    "            gray_scale = np.zeros_like(gray_scale_original)\n",
    "        else:\n",
    "            gray_scale = gray_scale_original.copy()\n",
    "            threshold = np.percentile(gray_scale, i)\n",
    "            gray_scale = gray_scale <= threshold\n",
    "\n",
    "        # Load and preprocess the image for CAM overlay\n",
    "        rgb_img = Image.open(img_path).convert('RGB')\n",
    "        rgb_img = np.array(rgb_img.resize((224, 224))) / 255.0\n",
    "        img_saliency = show_cam_on_image(rgb_img, gray_scale[0, :], use_rgb=True)\n",
    "\n",
    "        # Calculate predicted class and confidence score change\n",
    "        predicted_class = model(input_tensor).max(1)[-1]\n",
    "        # targets = [ClassifierOutputTarget(predicted_class)]\n",
    "        targets = [ClassifierOutputSoftmaxTarget(predicted_class)]\n",
    "        scores, scores_before, scores_after, visualizations = cam_metric(\n",
    "            input_tensor, gray_scale, targets, model, return_visualization=True\n",
    "        )\n",
    "\n",
    "        # Process the visualization for display and scoring\n",
    "        # score = scores[0]\n",
    "        # visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "        # visualization = deprocess_image(visualization)\n",
    "\n",
    "        # Save the visualization\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, filename))\n",
    "        # Image.fromarray(visualization).save(os.path.join(imputated_folder, f\"{os.path.splitext(filename)[0]}_{i}{os.path.splitext(filename)[1]}\"))\n",
    "        confidences.append(scores_after)\n",
    "\n",
    "        # Print individual results\n",
    "        # print(f\"Image: {filename}\")\n",
    "        print(f\"Step: {i}\\tConfidence: {scores_after}\")\n",
    "        # print(\"----------------------------------------\")\n",
    "    confidences = np.array(confidences).flatten()\n",
    "    fraction_revealed = np.linspace(0, 1, len(confidences))\n",
    "    auc = np.trapz(confidences, fraction_revealed)\n",
    "    count += 1\n",
    "    sum_auc += auc\n",
    "    print(f\"Image: {filename}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "print(f\"Average AUC: {auc/count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "score-cam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
